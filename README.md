# Local LangChain AI Demos

Two concise, production-ready examples showcasing **local LLM applications with LangChain + Ollama**.

## 1. [Chatbot Assistant with Persistent Memory](langchain/LangChain-v1-Ollama-Gradio-Chatbot-with-Memory-and-Tools-Demo.ipynb)

![Overview](langchain/img/cba-with-memory.png)

A fully local, tool-enabled chatbot with session-isolated, persistent memory.

**Highlights**
- ğŸ§  Persistent memory via LangGraph `MemorySaver`
- ğŸ”§ Tool calling (Wikipedia, Tavily, custom tools)
- ğŸ¤– Local LLMs via Ollama (e.g. `qwen2.5:3b`)
- ğŸ¨ Gradio UI with session state
- ğŸ”’ Privacy-first, offline-capable

**Use cases:** personal assistants, research bots, privacy-sensitive apps.

---

## 2. [LangChain RAG Demo](langchain/LangChain-v1-RAG-Demo-with-Ollama.ipynb)
A lightweight Retrieval-Augmented Generation (RAG) pipeline for grounded Q&A.

**Highlights**
- ğŸ“š In-memory vector store with semantic search
- âœ‚ï¸ Automatic document chunking + embeddings
- ğŸ¤– Ollama-powered LLM (Mistral 7B)
- ğŸ’¬ Single-turn and multi-turn chat modes
- ğŸ§© Pluggable embeddings (HF or fallback)

**Use cases:** document Q&A, knowledge assistants, RAG prototyping.

---

## Tech Stack
- LangChain (v1.0+)
- Ollama (local LLM inference)
- Gradio (chat UI)
- HuggingFace embeddings (optional)

**Goal:** demonstrate clean, modern patterns for building **local, private, LLM-powered systems** with memory, tools, and retrieval.
