{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b84a3af-4b58-46f5-9ca2-78b1a0e8a776",
   "metadata": {},
   "source": [
    "### Chatbot Assistant with Persistent Memory and Tool Use using LangChain v1.0+, Ollama, and Gradio v6.1.0\n",
    "\n",
    "This implementation showcases a fully local, tool-enabled conversational assistant built with LangChain v1.0+, powered by open-source large language models served via Ollama, and delivered through a Gradio v6.1.0 user interface. The architecture is designed for robustness, extensibility, and compatibility with non-OpenAI models while preserving conversational memory across turns.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Features and Design Updates\n",
    "\n",
    "### Local LLM Integration (Ollama)\n",
    "- Uses `langchain_ollama.ChatOllama` instead of `ChatOpenAI`\n",
    "- Runs entirely on local infrastructure via an Ollama server (`base_url=\"http://localhost:11434\"`)\n",
    "- Deterministic responses enabled by setting `temperature=0`\n",
    "- Model selection is configurable (default: `qwen2.5:3b`)\n",
    "\n",
    "### Agent-Based Tool Usage\n",
    "- Leverages LangChain’s `create_agent` abstraction\n",
    "- Accepts a custom `system_prompt` at agent creation time\n",
    "- Enables tool calling for open-source models that do not support OpenAI function calling\n",
    "- Supports arbitrary tools such as:\n",
    "  - Wikipedia search\n",
    "  - Weather lookups\n",
    "  - Custom user-defined tools\n",
    "\n",
    "### Persistent Conversation Memory\n",
    "- Uses `ChatMessageHistory` for structured and reliable memory management\n",
    "- Maintains a complete message history across turns\n",
    "- Injects full conversation context into each agent invocation\n",
    "- Allows the assistant to recall, reference, and reason over prior interactions\n",
    "\n",
    "### Explicit Message Construction\n",
    "- Manually constructs the message list using LangChain message objects\n",
    "- Replays stored messages from `ChatMessageHistory` on each request\n",
    "- Appends the current `HumanMessage` before invoking the agent\n",
    "- Extracts the final assistant response from the returned message list\n",
    "\n",
    "### Error Handling and Stability\n",
    "- Wraps agent invocation in a `try/except` block to handle local LLM failures\n",
    "- Logs full Python tracebacks for debugging and observability\n",
    "- Returns a graceful user-facing error message when exceptions occur\n",
    "\n",
    "### Gradio v6.1.0 User Interface\n",
    "- Built with `gr.Blocks` for layout flexibility\n",
    "- Includes:\n",
    "  - Scrollable chat window\n",
    "  - Multi-line text input\n",
    "  - Send and clear-history controls\n",
    "  - Example prompts for quick interaction\n",
    "- Displays live metadata about memory usage (message count)\n",
    "\n",
    "### UI and Memory State Separation\n",
    "- Internal conversation state is managed exclusively by LangChain (`ChatMessageHistory`)\n",
    "- Gradio chat state is maintained independently for rendering\n",
    "- Ensures clean separation between UI concerns and agent logic\n",
    "- Clear-history action resets both UI state and internal memory\n",
    "\n",
    "### Modular and Extensible Architecture\n",
    "- Tools are injected at initialization time\n",
    "- Model configuration is parameterized\n",
    "- UI creation, memory management, and agent execution are clearly separated\n",
    "- Suitable as a foundation for more advanced local AI assistants\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This project provides a production-ready reference architecture for building local, tool-augmented conversational agents with persistent memory using LangChain’s modern agent APIs. It avoids reliance on proprietary APIs or OpenAI-specific features, making it well suited for open-source LLM deployments, privacy-sensitive environments, and offline-first applications.\n",
    "\n",
    "*Based on the final project from the course **“Functions, Tools and Agents with LangChain”** by deeplearning(dot)ai* fully updated to LangChain v1+, with a new custom UI based on Gradio and a local LLM running via Ollama."
   ]
  },
  {
   "cell_type": "raw",
   "id": "4f100a7f-de23-4519-8bc2-069fd9b1c8e1",
   "metadata": {},
   "source": [
    "%pip install -qU langchain langchain-core langchain-community langchain-ollama wikipedia-api requests pydantic gradio param tavily-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae7ef52-edbe-4fed-bd51-46ffe63f8fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import param\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.agents import create_agent\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbfed4e-1537-458e-b9ed-6afd1b711475",
   "metadata": {},
   "outputs": [],
   "source": [
    "class chatbot_demo(param.Parameterized):\n",
    "    \n",
    "    def __init__(self, tools, model_name=\"qwen2.5:3b\", **params):\n",
    "        super(chatbot_demo, self).__init__(**params)\n",
    "        self.tools = tools\n",
    "        \n",
    "        # Use LangChain's ChatMessageHistory for better memory management\n",
    "        self.chat_history = ChatMessageHistory()\n",
    "        \n",
    "        # System message\n",
    "        self.system_prompt = \"You are a helpful, smart and jovial assistant who remembers the conversation history\"\n",
    "        \n",
    "        # Create llm with Ollama\n",
    "        self.model = ChatOllama(\n",
    "            model=model_name,\n",
    "            temperature=0.0, # increase for less deterministic results\n",
    "            base_url=\"http://localhost:11434\"\n",
    "        )\n",
    "        \n",
    "        # Create the agent using create_agent with system_prompt parameter\n",
    "        self.agent = create_agent(\n",
    "            model=self.model,\n",
    "            tools=self.tools,\n",
    "            system_prompt=self.system_prompt\n",
    "        )\n",
    "    \n",
    "    def process_chain(self, message, history):\n",
    "        \"\"\"\n",
    "        Process user message and return response with memory retention.\n",
    "        \n",
    "        Args:\n",
    "            message: Current user message\n",
    "            history: List of [user_msg, bot_msg] pairs from Gradio\n",
    "        \n",
    "        Returns:\n",
    "            Bot response string\n",
    "        \"\"\"\n",
    "        if not message:\n",
    "            return \"\"\n",
    "        \n",
    "        try:\n",
    "            # Build messages list including full history\n",
    "            messages = []\n",
    "            \n",
    "            # Get all messages from internal chat history\n",
    "            for msg in self.chat_history.messages:\n",
    "                messages.append(msg)\n",
    "            \n",
    "            # Add current user message\n",
    "            messages.append(HumanMessage(content=message))\n",
    "            \n",
    "            # Invoke agent with full message history\n",
    "            result = self.agent.invoke({\"messages\": messages})\n",
    "            \n",
    "            # Extract bot response (last message in the result)\n",
    "            bot_response = result[\"messages\"][-1].content\n",
    "            \n",
    "            # Store in chat history for future turns\n",
    "            self.chat_history.add_user_message(message)\n",
    "            self.chat_history.add_ai_message(bot_response)\n",
    "            \n",
    "            return bot_response\n",
    "            \n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            error_details = traceback.format_exc()\n",
    "            print(f\"Error details:\\n{error_details}\")\n",
    "            return f\"Sorry, I encountered an error: {str(e)}\"\n",
    "    \n",
    "    def clear_history(self):\n",
    "        \"\"\"Clear conversation memory\"\"\"\n",
    "        self.chat_history.clear()\n",
    "        return None\n",
    "    \n",
    "    def get_history_summary(self):\n",
    "        \"\"\"Get a summary of conversation history\"\"\"\n",
    "        return f\"Total messages in history: {len(self.chat_history.messages)}\"\n",
    "    \n",
    "    def create_ui(self):\n",
    "        \"\"\"Create and return Gradio ChatInterface\"\"\"\n",
    "        \n",
    "        with gr.Blocks() as demo:\n",
    "            gr.Markdown(\"# Smart and Jovial Assistant Chatbot\")\n",
    "            gr.Markdown(\"Ask me anything! I'll remember our conversation.\")\n",
    "            \n",
    "            chatbot = gr.Chatbot(height=500)\n",
    "\n",
    "            msg = gr.Textbox(\n",
    "                placeholder=\"Type your message here...\",\n",
    "                label=\"Message\",\n",
    "                lines=2\n",
    "            )\n",
    "            \n",
    "            with gr.Row():\n",
    "                submit = gr.Button(\"Send\", variant=\"primary\")\n",
    "                clear = gr.Button(\"Clear History\")\n",
    "            \n",
    "            # History info display\n",
    "            history_info = gr.Markdown(\"Messages in memory: 0\")\n",
    "            \n",
    "            # Examples\n",
    "            gr.Examples(\n",
    "                examples=[\n",
    "                    \"What can you help me with?\",\n",
    "                    \"Search Wikipedia for Deep Learning\",\n",
    "                    \"What's the temperature in Paris? (latitude: 48.857817, longitude: 2.295198)\",\n",
    "                    \"What did we talk about earlier?\"\n",
    "                ],\n",
    "                inputs=msg\n",
    "            )\n",
    "            \n",
    "            def respond(message, chat_history):\n",
    "                if not message:\n",
    "                    return \"\", chat_history, self.get_history_summary()\n",
    "            \n",
    "                if chat_history is None:\n",
    "                    chat_history = []\n",
    "            \n",
    "                bot_message = self.process_chain(message, chat_history)\n",
    "                \n",
    "                # Format for Gradio's \"messages\" type\n",
    "                chat_history = chat_history + [\n",
    "                    {\"role\": \"user\", \"content\": message},\n",
    "                    {\"role\": \"assistant\", \"content\": bot_message}\n",
    "                ]\n",
    "            \n",
    "                return \"\", chat_history, self.get_history_summary()\n",
    "            \n",
    "            \n",
    "            def clear_all():\n",
    "                self.clear_history()\n",
    "                return [], \"Messages in memory: 0\"\n",
    "\n",
    "            \n",
    "            # Event handlers\n",
    "            submit.click(respond, [msg, chatbot], [msg, chatbot, history_info])\n",
    "            msg.submit(respond, [msg, chatbot], [msg, chatbot, history_info])\n",
    "            clear.click(clear_all, None, [chatbot, history_info])\n",
    "        \n",
    "        return demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e40bbfc-61b4-441c-bf83-b429447fd387",
   "metadata": {},
   "source": [
    "### Tools available to the chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367e478c-61ba-4a61-9f3c-c3feadc6e284",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "import wikipedia\n",
    "import requests\n",
    "import datetime\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b80ec8-5f65-4cbc-a029-646260da60e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your tool\n",
    "@tool\n",
    "def create_your_tool(query: str) -> str:\n",
    "    \"\"\"This function can do whatever you would like once you fill it in \"\"\"\n",
    "    print(type(query))\n",
    "    return query[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fcbe19-852c-4acd-97f7-634db3a7725f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wikipedia tool\n",
    "@tool\n",
    "def search_wikipedia(query: str) -> str:\n",
    "    \"\"\"Run Wikipedia search and get page summaries.\"\"\"\n",
    "    page_titles = wikipedia.search(query)\n",
    "    summaries = []\n",
    "    for page_title in page_titles[:3]:\n",
    "        try:\n",
    "            wiki_page = wikipedia.page(title=page_title, auto_suggest=False)\n",
    "            summaries.append(f\"Page: {page_title}\\nSummary: {wiki_page.summary}\")\n",
    "        except (\n",
    "            wikipedia.exceptions.PageError,\n",
    "            wikipedia.exceptions.DisambiguationError,\n",
    "        ):\n",
    "            pass\n",
    "    if not summaries:\n",
    "        return \"No good Wikipedia Search Result was found\"\n",
    "    return \"\\n\\n\".join(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b25f465-54f7-45d1-b0d3-86e5c32fd3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import requests\n",
    "from typing import Optional\n",
    "from pydantic import BaseModel, Field, field_validator\n",
    "from langchain.tools import tool\n",
    "\n",
    "\n",
    "class OpenMeteoInput(BaseModel):\n",
    "    city: Optional[str] = Field(\n",
    "        None, \n",
    "        description=\"City or location name (e.g., 'Paris', 'Nevada', 'Cracow, 'Fuerteventura')\"\n",
    "    )\n",
    "    country: Optional[str] = Field(\n",
    "        None, \n",
    "        description=\"Country name or ISO code (e.g., 'Spain', 'USA', 'Poland', 'ES'). Optional but improves accuracy.\"\n",
    "    )\n",
    "    latitude: Optional[float] = Field(\n",
    "        None, \n",
    "        description=\"Latitude coordinate. Only use if explicitly provided by user.\"\n",
    "    )\n",
    "    longitude: Optional[float] = Field(\n",
    "        None, \n",
    "        description=\"Longitude coordinate. Only use if explicitly provided by user.\"\n",
    "    )\n",
    "\n",
    "@tool(args_schema=OpenMeteoInput)\n",
    "def get_weather_conditions(\n",
    "    city: Optional[str] = None,\n",
    "    country: Optional[str] = None,\n",
    "    latitude: Optional[float] = None,\n",
    "    longitude: Optional[float] = None\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Get current weather conditions (temperature, precipitation, wind, humidity) for any location.\n",
    "    Use this whenever you need weather information to answer a question, whether explicitly asked or contextually needed.\n",
    "    Automatically geocodes city names to coordinates.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Geocode if coordinates not provided\n",
    "    if latitude is None or longitude is None:\n",
    "        if not city:\n",
    "            return \"Error: Please provide either a location name or both latitude and longitude.\"\n",
    "        \n",
    "        location_query = f\"{city}, {country}\" if country else city\n",
    "        \n",
    "        geocode_url = \"https://nominatim.openstreetmap.org/search\"\n",
    "        geocode_params = {\n",
    "            'q': location_query,\n",
    "            'format': 'json',\n",
    "            'limit': 1\n",
    "        }\n",
    "        headers = {'User-Agent': 'LangChain-Weather-Bot/1.0'}\n",
    "        \n",
    "        try:\n",
    "            geo_response = requests.get(geocode_url, params=geocode_params, headers=headers, timeout=10)\n",
    "            geo_response.raise_for_status()\n",
    "            geo_data = geo_response.json()\n",
    "            \n",
    "            if not geo_data:\n",
    "                return f\"Error: Could not find coordinates for '{location_query}'.\"\n",
    "            \n",
    "            latitude = float(geo_data[0]['lat'])\n",
    "            longitude = float(geo_data[0]['lon'])\n",
    "            location_name = geo_data[0].get('display_name', location_query)\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Error geocoding location: {str(e)}\"\n",
    "    else:\n",
    "        location_name = f\"coordinates ({latitude}, {longitude})\"\n",
    "    \n",
    "    # Fetch weather data\n",
    "    BASE_URL = \"https://api.open-meteo.com/v1/forecast\"\n",
    "    \n",
    "    params = {\n",
    "        'latitude': latitude,\n",
    "        'longitude': longitude,\n",
    "        'current': 'temperature_2m,relative_humidity_2m,precipitation,rain,weather_code,wind_speed_10m,wind_direction_10m',\n",
    "        'forecast_days': 1,\n",
    "        'temperature_unit': 'celsius',\n",
    "        'wind_speed_unit': 'kmh',\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(BASE_URL, params=params, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        results = response.json()\n",
    "    except Exception as e:\n",
    "        return f\"Error fetching weather data: {str(e)}\"\n",
    "    \n",
    "    # Parse current weather\n",
    "    current = results.get('current', {})\n",
    "    temp = current.get('temperature_2m', 'N/A')\n",
    "    humidity = current.get('relative_humidity_2m', 'N/A')\n",
    "    precipitation = current.get('precipitation', 0)\n",
    "    rain = current.get('rain', 0)\n",
    "    wind_speed = current.get('wind_speed_10m', 'N/A')\n",
    "    wind_direction = current.get('wind_direction_10m', 'N/A')\n",
    "    weather_code = current.get('weather_code', 0)\n",
    "    \n",
    "    # Decode weather condition\n",
    "    weather_descriptions = {\n",
    "        0: \"Clear sky\", 1: \"Mainly clear\", 2: \"Partly cloudy\", 3: \"Overcast\",\n",
    "        45: \"Foggy\", 48: \"Depositing rime fog\",\n",
    "        51: \"Light drizzle\", 53: \"Moderate drizzle\", 55: \"Dense drizzle\",\n",
    "        61: \"Slight rain\", 63: \"Moderate rain\", 65: \"Heavy rain\",\n",
    "        71: \"Slight snow\", 73: \"Moderate snow\", 75: \"Heavy snow\", 77: \"Snow grains\",\n",
    "        80: \"Slight rain showers\", 81: \"Moderate rain showers\", 82: \"Violent rain showers\",\n",
    "        85: \"Slight snow showers\", 86: \"Heavy snow showers\",\n",
    "        95: \"Thunderstorm\", 96: \"Thunderstorm with slight hail\", 99: \"Thunderstorm with heavy hail\"\n",
    "    }\n",
    "    weather_condition = weather_descriptions.get(weather_code, \"Unknown\")\n",
    "    \n",
    "    # Build report\n",
    "    report = (\n",
    "        f\"Weather in {location_name}:\\n\"\n",
    "        f\"Temperature: {temp}°C\\n\"\n",
    "        f\"Conditions: {weather_condition}\\n\"\n",
    "        f\"Humidity: {humidity}%\\n\"\n",
    "        f\"Wind: {wind_speed} km/h from {wind_direction}°\\n\"\n",
    "        f\"Precipitation: {precipitation} mm (rain: {rain} mm)\"\n",
    "    )\n",
    "    \n",
    "    return report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11465554-a49f-42fa-8d01-d09d2f73a75b",
   "metadata": {},
   "source": [
    "### User Interface (Gradio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd89840b-9add-4041-a20d-03a998384f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Chatbot UI \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Create agent with tools\n",
    "    tools = [create_your_tool,search_wikipedia, get_weather_conditions]\n",
    "    chatbot = chatbot_demo(tools=tools, model_name=\"qwen2.5:3b\")\n",
    "    \n",
    "    # Launch interface\n",
    "    interface = chatbot.create_ui()\n",
    "    interface.launch(share=False, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8265e7-ce90-423a-89ae-04d575744208",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
