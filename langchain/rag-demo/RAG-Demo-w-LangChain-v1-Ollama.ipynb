{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7029d550-4a0f-4233-8fcf-8161f5645ffe",
   "metadata": {},
   "source": [
    "# LangChain RAG Demo with Ollama\n",
    "\n",
    "A lightweight Retrieval-Augmented Generation (RAG) system demonstrating context-aware question answering with local LLM deployment.\n",
    "\n",
    "## Main Features\n",
    "\n",
    "- **Document Retrieval**: In-memory vector store with semantic search\n",
    "- **Local LLM Integration**: Ollama-powered inference (Mistral 7B)\n",
    "- **Conversation Modes**: Single-turn Q&A and multi-turn chat\n",
    "- **Flexible Embeddings**: HuggingFace embeddings with FakeEmbeddings fallback\n",
    "- **Text Processing**: Automatic document chunking with overlap\n",
    "\n",
    "## Architecture\n",
    "\n",
    "```\n",
    "Query → Retriever → Vector Search → Context Extraction\n",
    "                                           ↓\n",
    "User Query + Context → LLM Prompt → Ollama (Mistral) → Response\n",
    "```\n",
    "\n",
    "**RAG Pipeline**:\n",
    "1. Documents split into chunks (100 chars, 20 overlap)\n",
    "2. Chunks embedded and stored in vector database\n",
    "3. User query retrieves top-2 relevant chunks\n",
    "4. Context + query sent to LLM for grounded response\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "**Core**:\n",
    "- `langchain` - Framework orchestration\n",
    "- `langchain-ollama` - Ollama LLM integration\n",
    "- `langchain-text-splitters` - Document chunking\n",
    "\n",
    "**Optional (for embeddings)**:\n",
    "- `langchain-huggingface` - Sentence embeddings\n",
    "- `sentence-transformers` - Embedding models\n",
    "- `transformers` + `torch` - Model support\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "```bash\n",
    "# Install and run Ollama\n",
    "ollama serve\n",
    "ollama pull mistral:7b\n",
    "```\n",
    "\n",
    "## Usage\n",
    "\n",
    "**Single-turn**: `retrieve_and_respond(query)` - Direct Q&A  \n",
    "**Multi-turn**: `ConversationPipeline.chat(query)` - Conversational context\n",
    "\n",
    "## Key Components\n",
    "\n",
    "- **LLM**: ChatOllama (Mistral 7B, temp=0.7)\n",
    "- **Vector Store**: InMemoryVectorStore (ephemeral)\n",
    "- **Retriever**: Top-2 similarity search\n",
    "- **Embeddings**: MiniLM-L6-v2 (384 dims) or fake fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaa95a3e-ad16-43b2-b373-a83b406f6dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core LangChain and related packages\n",
    "# %pip install -qU langchain langchain-ollama langchain-text-splitters\n",
    "\n",
    "# Optional: HuggingFace embeddings for real vector support\n",
    "# %pip install -qU langchain_huggingface\n",
    "\n",
    "# Optional: Transformers and PyTorch for local models\n",
    "# %pip install -qU transformers torch\n",
    "\n",
    "# Optional: SentenceTransformers for sentence embeddings\n",
    "# %pip install -qU sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0551f3c6-e427-4655-9aa2-d904c4682a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b44de9-71b1-40ca-a984-e74850994b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Initialize Ollama Chat Model\n",
    "# Ensure ollama is running :\n",
    "#    ollama serve\n",
    "#    ollama pull mistral:7b\n",
    "llm = ChatOllama(\n",
    "    model=\"mistral:7b\",  \n",
    "    temperature=0.7,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc674084-dc45-4779-9cf3-fd70208b3b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Prepare Documents\n",
    "documents = [\n",
    "    \"Large Language Models (LLMs) are AI systems trained on massive text datasets to understand and generate human language. \"\n",
    "    \"They are commonly built using Transformer architectures, which allow them to process context across long sequences of text.\",\n",
    "\n",
    "    \"Natural Language Processing (NLP) focuses on enabling computers to interpret and work with human language. \"\n",
    "    \"Core NLP tasks include text classification, question answering, sentiment analysis, summarization, and translation.\",\n",
    "\n",
    "    \"Python is widely used for building and deploying LLM and NLP systems. \"\n",
    "    \"Popular libraries and frameworks include Hugging Face Transformers for model usage, tokenizers for text processing, \"\n",
    "    \"and vector databases for retrieval-augmented generation (RAG) workflows.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7e51be-26d9-4eaf-bfa2-2609fc552850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Split Documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    ")\n",
    "chunks = text_splitter.split_text(\"\\n\".join(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ef4bd5-01d0-47b1-a1df-f69f2bba45c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Create Vector Store\n",
    "# Fallback embeddings\n",
    "try:\n",
    "    from langchain_huggingface import HuggingFaceEmbeddings\n",
    "    import torch  # Auto-detected by sentence-transformers\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'}\n",
    "    )\n",
    "except (ImportError, ModuleNotFoundError, RuntimeError) as e:  # Add RuntimeError for GPU issues\n",
    "    from langchain_core.embeddings import FakeEmbeddings\n",
    "    print(f\"HuggingFace unavailable ({e}), using FakeEmbeddings fallback.\")\n",
    "    embeddings = FakeEmbeddings(size=384)  # Match MiniLM dimension\n",
    "    \n",
    "vectorstore = InMemoryVectorStore.from_texts(chunks, embeddings)\n",
    "retriever = vectorstore.as_retriever(k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1e4ba9-d9cd-436b-a61a-70968402e377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Define RAG Prompt\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "        \"You are a helpful assistant. Prioritize answering using the provided context. \"\n",
    "        \"If the context contains the answer, rely on it strictly. \"\n",
    "        \"If the context does not contain the answer, you may use your own verified, \"\n",
    "        \"parametric knowledge — but do NOT make up facts. \"\n",
    "        \"If you are not confident or the information is unknown, say \"\n",
    "        \"'I don't know' or 'The context does not provide this information.'\\n\\n\"\n",
    "        \"Context: {context}\"\n",
    "    ),\n",
    "    (\"human\", \"{query}\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad39333-2b1c-4f67-b1ff-647d0105c0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Simple RAG Function\n",
    "def retrieve_and_respond(query: str) -> str:\n",
    "    \"\"\"Retrieve docs → format prompt → invoke LLM\"\"\"\n",
    "    context_docs = retriever.invoke(query)\n",
    "    context = \"\\n\".join([doc.page_content for doc in context_docs])\n",
    "    \n",
    "    messages = prompt.format_messages(context=context, query=query)\n",
    "    response = llm.invoke(messages)\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9539b5e3-3254-4a45-ac93-7fd9c2ee20a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Multi-turn Conversation\n",
    "class ConversationPipeline:\n",
    "    def __init__(self, llm, retriever, prompt):\n",
    "        self.llm = llm\n",
    "        self.retriever = retriever\n",
    "        self.prompt = prompt\n",
    "        self.chat_history = []\n",
    "    \n",
    "    def chat(self, user_query: str) -> str:\n",
    "        \"\"\"Handle multi-turn conversation\"\"\"\n",
    "        context_docs = self.retriever.invoke(user_query)\n",
    "        context = \"\\n\".join([doc.page_content for doc in context_docs])\n",
    "        \n",
    "        # Format messages with context and conversation history\n",
    "        messages = prompt.format_messages(context=context, query=user_query)\n",
    "        \n",
    "        response = self.llm.invoke(messages)\n",
    "        response_text = response.content\n",
    "        \n",
    "        # Store in history\n",
    "        self.chat_history.append({\"user\": user_query, \"assistant\": response_text})\n",
    "        \n",
    "        return response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64720b05-c6f9-4e39-ac92-e7915f25dab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Run Pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=== Single-turn RAG ===\")\n",
    "    result = retrieve_and_respond(\"What is a Large Language Model?\")\n",
    "    print(f\"Response: {result}\\n\")\n",
    "    \n",
    "    print(\"=== Multi-turn Conversation ===\")\n",
    "    conversation = ConversationPipeline(llm, retriever, prompt)\n",
    "    \n",
    "    queries = [\n",
    "        \"What is a Large Language Model?\",\n",
    "        \"What are common tasks in Natural Language Processing?\",\n",
    "        \"Can I use Python to build and deploy LLM applications?\"\n",
    "    ]\n",
    "    \n",
    "    for query in queries:\n",
    "        response = conversation.chat(query)\n",
    "        print(f\"User: {query}\")\n",
    "        print(f\"Assistant: {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c48bec7-917d-4def-999c-a941cdb55aa1",
   "metadata": {},
   "source": [
    "### With Tools (Function Calling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97fb922-8a8e-4636-a057-9b65df999133",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "@tool  # Docstring is mandatory as it becomes the tool description given to the LLM\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two integers and return the result.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "llm_with_tools = llm.bind_tools([multiply])\n",
    "\n",
    "# Ask model\n",
    "ai_msg = llm_with_tools.invoke([HumanMessage(\"What is 5 times 3?\")])\n",
    "\n",
    "# Execute tool\n",
    "tool_call = ai_msg.tool_calls[0]\n",
    "result = multiply.invoke(tool_call[\"args\"])\n",
    "\n",
    "print(\"Tool result:\", result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc10e59-d13c-493a-a73c-7b8650ae3532",
   "metadata": {},
   "source": [
    "### Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9397ae8d-360c-4a50-9181-df0c12cfcdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Streaming ===\")\n",
    "\n",
    "for chunk in llm.stream(\"Tell me about Large Language Models\"):\n",
    "    print(chunk.content, end=\"\", flush=True)\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
