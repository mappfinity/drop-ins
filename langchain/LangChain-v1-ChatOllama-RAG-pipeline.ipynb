{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02f2ad1b-0e7b-4568-a357-7af63c96b4d2",
   "metadata": {},
   "source": [
    "### LangChain RAG demo with Ollama, in-memory vectors, tool use and streaming\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaa95a3e-ad16-43b2-b373-a83b406f6dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core LangChain and related packages\n",
    "# %pip install -qU langchain langchain-ollama langchain-text-splitters\n",
    "\n",
    "# Optional: HuggingFace embeddings for real vector support\n",
    "# %pip install -qU langchain_huggingface\n",
    "\n",
    "# Optional: Transformers and PyTorch for local models\n",
    "# %pip install -qU transformers torch\n",
    "\n",
    "# Optional: SentenceTransformers for sentence embeddings\n",
    "# %pip install -qU sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0551f3c6-e427-4655-9aa2-d904c4682a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6b44de9-71b1-40ca-a984-e74850994b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Initialize Ollama Chat Model\n",
    "# Ensure ollama is running :\n",
    "#    ollama serve\n",
    "#    ollama pull mistral:7b\n",
    "llm = ChatOllama(\n",
    "    model=\"mistral:7b\",  \n",
    "    temperature=0.7,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc674084-dc45-4779-9cf3-fd70208b3b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Prepare Documents\n",
    "documents = [\n",
    "    \"Large Language Models (LLMs) are AI systems trained on massive text datasets to understand and generate human language. \"\n",
    "    \"They are commonly built using Transformer architectures, which allow them to process context across long sequences of text.\",\n",
    "\n",
    "    \"Natural Language Processing (NLP) focuses on enabling computers to interpret and work with human language. \"\n",
    "    \"Core NLP tasks include text classification, question answering, sentiment analysis, summarization, and translation.\",\n",
    "\n",
    "    \"Python is widely used for building and deploying LLM and NLP systems. \"\n",
    "    \"Popular libraries and frameworks include Hugging Face Transformers for model usage, tokenizers for text processing, \"\n",
    "    \"and vector databases for retrieval-augmented generation (RAG) workflows.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f7e51be-26d9-4eaf-bfa2-2609fc552850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Split Documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    ")\n",
    "chunks = text_splitter.split_text(\"\\n\".join(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36ef4bd5-01d0-47b1-a1df-f69f2bba45c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Create Vector Store\n",
    "# Fallback embeddings\n",
    "try:\n",
    "    from langchain_huggingface import HuggingFaceEmbeddings\n",
    "    import torch  # Auto-detected by sentence-transformers\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'}\n",
    "    )\n",
    "except (ImportError, ModuleNotFoundError, RuntimeError) as e:  # Add RuntimeError for GPU issues\n",
    "    from langchain_core.embeddings import FakeEmbeddings\n",
    "    print(f\"HuggingFace unavailable ({e}), using FakeEmbeddings fallback.\")\n",
    "    embeddings = FakeEmbeddings(size=384)  # Match MiniLM dimension\n",
    "    \n",
    "vectorstore = InMemoryVectorStore.from_texts(chunks, embeddings)\n",
    "retriever = vectorstore.as_retriever(k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a1e4ba9-d9cd-436b-a61a-70968402e377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Define RAG Prompt\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "        \"You are a helpful assistant. Prioritize answering using the provided context. \"\n",
    "        \"If the context contains the answer, rely on it strictly. \"\n",
    "        \"If the context does not contain the answer, you may use your own verified, \"\n",
    "        \"parametric knowledge — but do NOT make up facts. \"\n",
    "        \"If you are not confident or the information is unknown, say \"\n",
    "        \"'I don't know' or 'The context does not provide this information.'\\n\\n\"\n",
    "        \"Context: {context}\"\n",
    "    ),\n",
    "    (\"human\", \"{query}\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ad39333-2b1c-4f67-b1ff-647d0105c0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Simple RAG Function\n",
    "def retrieve_and_respond(query: str) -> str:\n",
    "    \"\"\"Retrieve docs → format prompt → invoke LLM\"\"\"\n",
    "    context_docs = retriever.invoke(query)\n",
    "    context = \"\\n\".join([doc.page_content for doc in context_docs])\n",
    "    \n",
    "    messages = prompt.format_messages(context=context, query=query)\n",
    "    response = llm.invoke(messages)\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9539b5e3-3254-4a45-ac93-7fd9c2ee20a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Multi-turn Conversation\n",
    "class ConversationPipeline:\n",
    "    def __init__(self, llm, retriever, prompt):\n",
    "        self.llm = llm\n",
    "        self.retriever = retriever\n",
    "        self.prompt = prompt\n",
    "        self.chat_history = []\n",
    "    \n",
    "    def chat(self, user_query: str) -> str:\n",
    "        \"\"\"Handle multi-turn conversation\"\"\"\n",
    "        context_docs = self.retriever.invoke(user_query)\n",
    "        context = \"\\n\".join([doc.page_content for doc in context_docs])\n",
    "        \n",
    "        # Format messages with context and conversation history\n",
    "        messages = prompt.format_messages(context=context, query=user_query)\n",
    "        \n",
    "        response = self.llm.invoke(messages)\n",
    "        response_text = response.content\n",
    "        \n",
    "        # Store in history\n",
    "        self.chat_history.append({\"user\": user_query, \"assistant\": response_text})\n",
    "        \n",
    "        return response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64720b05-c6f9-4e39-ac92-e7915f25dab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Single-turn RAG ===\n",
      "Response:  A Large Language Model (LLM) is an AI system trained on extensive text datasets to understand and generate human language. It's designed to process context across long sequences of text using architectures such as Transformers. Core NLP tasks for LLMs include text classification and question answering.\n",
      "\n",
      "=== Multi-turn Conversation ===\n",
      "User: What is a Large Language Model?\n",
      "Assistant:  A Large Language Model (LLM) is an AI system trained on vast amounts of text data to understand and generate human language. It utilizes advanced architectures like the Transformer model, which enables it to process context across long sequences of text. Core Natural Language Processing (NLP) tasks that LLMs can perform include text classification and question answering.\n",
      "\n",
      "User: What are common tasks in Natural Language Processing?\n",
      "Assistant:  Common tasks in Natural Language Processing (NLP) include:\n",
      "1. Text Classification: This involves categorizing texts into predefined classes, such as spam vs non-spam emails or positive vs negative reviews.\n",
      "2. Question Answering: This task aims to extract information or answers from text in response to a user's question.\n",
      "3. Named Entity Recognition (NER): This involves identifying and categorizing named entities such as people, organizations, locations, etc., within the text.\n",
      "4. Sentiment Analysis: This task is used to determine the emotional tone behind words, for example, positive, negative or neutral sentiment.\n",
      "5. Machine Translation: This task involves converting text from one language to another.\n",
      "6. Text Summarization: This task involves condensing a large amount of text into a shorter summary while still retaining the main ideas.\n",
      "7. Information Extraction (IE): This task involves automatically extracting structured information from unstructured text.\n",
      "\n",
      "User: Can I use Python to build and deploy LLM applications?\n",
      "Assistant:  Yes, based on the provided context, you can use Python for building and deploying Large Language Model (LLM) applications as it is widely used for such purposes.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Run Pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=== Single-turn RAG ===\")\n",
    "    result = retrieve_and_respond(\"What is a Large Language Model?\")\n",
    "    print(f\"Response: {result}\\n\")\n",
    "    \n",
    "    print(\"=== Multi-turn Conversation ===\")\n",
    "    conversation = ConversationPipeline(llm, retriever, prompt)\n",
    "    \n",
    "    queries = [\n",
    "        \"What is a Large Language Model?\",\n",
    "        \"What are common tasks in Natural Language Processing?\",\n",
    "        \"Can I use Python to build and deploy LLM applications?\"\n",
    "    ]\n",
    "    \n",
    "    for query in queries:\n",
    "        response = conversation.chat(query)\n",
    "        print(f\"User: {query}\")\n",
    "        print(f\"Assistant: {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c48bec7-917d-4def-999c-a941cdb55aa1",
   "metadata": {},
   "source": [
    "### With Tools (Function Calling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f97fb922-8a8e-4636-a057-9b65df999133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool result: 15\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "@tool  # Docstring is mandatory as it becomes the tool description given to the LLM\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two integers and return the result.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "llm_with_tools = llm.bind_tools([multiply])\n",
    "\n",
    "# Ask model\n",
    "ai_msg = llm_with_tools.invoke([HumanMessage(\"What is 5 times 3?\")])\n",
    "\n",
    "# Execute tool\n",
    "tool_call = ai_msg.tool_calls[0]\n",
    "result = multiply.invoke(tool_call[\"args\"])\n",
    "\n",
    "print(\"Tool result:\", result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc10e59-d13c-493a-a73c-7b8650ae3532",
   "metadata": {},
   "source": [
    "### Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9397ae8d-360c-4a50-9181-df0c12cfcdcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Streaming ===\n",
      " Large Language Models (LLMs) are a type of artificial intelligence (AI) model designed to process and generate human-like text. They are trained on vast amounts of internet text, learning from it to generate responses that mimic the way people talk and write.\n",
      "\n",
      "The most popular type of large language models is based on Transformer architecture, introduced by Google's paper \"Attention is All You Need\" in 2017. These models use self-attention mechanisms to understand the context of words within a sentence, making them capable of generating coherent and contextually appropriate responses.\n",
      "\n",
      "Examples of large language models include Meena, T5 (Text-to-Text Transfer Transformer), BERT (Bidirectional Encoder Representations from Transformers), and GPT-3 (Generative Pretrained Transformer 3) developed by OpenAI. These models have been trained on a vast amount of text data, allowing them to generate responses that are both informative and engaging.\n",
      "\n",
      "Large language models have many applications in areas such as chatbots, content generation, summarization, translation, and question-answering systems. However, due to their size and complexity, they require significant computational resources for training and deployment. Additionally, there is ongoing research and debate around the ethical implications of these models, including concerns about bias and misinformation.\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Streaming ===\")\n",
    "\n",
    "for chunk in llm.stream(\"Tell me about Large Language Models\"):\n",
    "    print(chunk.content, end=\"\", flush=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30cff0d-0644-4594-86a2-46848ae9ef38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
