{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaa95a3e-ad16-43b2-b373-a83b406f6dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install -qU langchain langchain-ollama langchain-text-splitters langchain_huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f2ad1b-0e7b-4568-a357-7af63c96b4d2",
   "metadata": {},
   "source": [
    "### RAG Pipeline with langchain_ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0551f3c6-e427-4655-9aa2-d904c4682a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6b44de9-71b1-40ca-a984-e74850994b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Initialize Ollama Chat Model\n",
    "# Ensure ollama is running :\n",
    "#    ollama serve\n",
    "#    ollama pull mistral:7b\n",
    "llm = ChatOllama(\n",
    "    model=\"mistral:7b\",  \n",
    "    temperature=0.7,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc674084-dc45-4779-9cf3-fd70208b3b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Prepare Documents\n",
    "documents = [\n",
    "    \"Large Language Models (LLMs) are AI systems trained on massive text datasets to understand and generate human language. \"\n",
    "    \"They are commonly built using Transformer architectures, which allow them to process context across long sequences of text.\",\n",
    "\n",
    "    \"Natural Language Processing (NLP) focuses on enabling computers to interpret and work with human language. \"\n",
    "    \"Core NLP tasks include text classification, question answering, sentiment analysis, summarization, and translation.\",\n",
    "\n",
    "    \"Python is widely used for building and deploying LLM and NLP systems. \"\n",
    "    \"Popular libraries and frameworks include Hugging Face Transformers for model usage, tokenizers for text processing, \"\n",
    "    \"and vector databases for retrieval-augmented generation (RAG) workflows.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f7e51be-26d9-4eaf-bfa2-2609fc552850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Split Documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    ")\n",
    "chunks = text_splitter.split_text(\"\\n\".join(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36ef4bd5-01d0-47b1-a1df-f69f2bba45c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Create Vector Store\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectorstore = InMemoryVectorStore.from_texts(chunks, embeddings)\n",
    "retriever = vectorstore.as_retriever(k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a1e4ba9-d9cd-436b-a61a-70968402e377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Define RAG Prompt\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "        \"You are a helpful assistant. Prioritize answering using the provided context. \"\n",
    "        \"If the context contains the answer, rely on it strictly. \"\n",
    "        \"If the context does not contain the answer, you may use your own verified, \"\n",
    "        \"parametric knowledge — but do NOT make up facts. \"\n",
    "        \"If you are not confident or the information is unknown, say \"\n",
    "        \"'I don't know' or 'The context does not provide this information.'\\n\\n\"\n",
    "        \"Context: {context}\"\n",
    "    ),\n",
    "    (\"human\", \"{query}\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ad39333-2b1c-4f67-b1ff-647d0105c0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Simple RAG Function\n",
    "def retrieve_and_respond(query: str) -> str:\n",
    "    \"\"\"Retrieve docs → format prompt → invoke LLM\"\"\"\n",
    "    context_docs = retriever.invoke(query)\n",
    "    context = \"\\n\".join([doc.page_content for doc in context_docs])\n",
    "    \n",
    "    messages = prompt.format_messages(context=context, query=query)\n",
    "    response = llm.invoke(messages)\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9539b5e3-3254-4a45-ac93-7fd9c2ee20a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Multi-turn Conversation\n",
    "class ConversationPipeline:\n",
    "    def __init__(self, llm, retriever, prompt):\n",
    "        self.llm = llm\n",
    "        self.retriever = retriever\n",
    "        self.prompt = prompt\n",
    "        self.chat_history = []\n",
    "    \n",
    "    def chat(self, user_query: str) -> str:\n",
    "        \"\"\"Handle multi-turn conversation\"\"\"\n",
    "        context_docs = self.retriever.invoke(user_query)\n",
    "        context = \"\\n\".join([doc.page_content for doc in context_docs])\n",
    "        \n",
    "        # Format messages with context and conversation history\n",
    "        messages = prompt.format_messages(context=context, query=user_query)\n",
    "        \n",
    "        response = self.llm.invoke(messages)\n",
    "        response_text = response.content\n",
    "        \n",
    "        # Store in history\n",
    "        self.chat_history.append({\"user\": user_query, \"assistant\": response_text})\n",
    "        \n",
    "        return response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64720b05-c6f9-4e39-ac92-e7915f25dab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Single-turn RAG ===\n",
      "Response:  A Large Language Model (LLM) is an AI system trained on extensive text datasets to comprehend and generate human language. These models are often built using Transformer architectures, enabling them to process context across long sequences of text. Common core Natural Language Processing (NLP) tasks for LLMs include text classification and question answering.\n",
      "\n",
      "=== Multi-turn Conversation ===\n",
      "User: What is a Large Language Model?\n",
      "Assistant:  A Large Language Model (LLM) is an Artificial Intelligence system trained on vast text datasets to understand and generate human language. These models are typically built using Transformer architectures, allowing them to process context across long sequences of text. Core tasks for LLMs include text classification and question answering.\n",
      "\n",
      "User: What are common tasks in Natural Language Processing?\n",
      "Assistant:  Common tasks in Natural Language Processing (NLP) include:\n",
      "1. Text Classification: This involves categorizing texts into predefined classes or topics, such as spam detection or sentiment analysis.\n",
      "2. Question Answering: This task aims to find the answer to a question within a given text corpus.\n",
      "3. Language Translation: This involves converting text from one language to another while retaining the original meaning.\n",
      "4. Named Entity Recognition (NER): This task identifies and categorizes named entities in the text, such as people, organizations, locations, etc.\n",
      "5. Sentiment Analysis: This involves determining the emotional tone behind the text, whether it's positive, negative, or neutral.\n",
      "6. Part-of-Speech Tagging (POS): This task identifies the grammatical role of each word in a sentence, such as noun, verb, adjective, etc.\n",
      "7. Dependency Parsing: This involves analyzing the grammatical structure of a sentence and identifying how its words are related to one another.\n",
      "8. Coreference Resolution: This task identifies when multiple expressions within a text refer to the same entity.\n",
      "9. Text Summarization: This involves condensing a large amount of text into a shorter summary while retaining its main points.\n",
      "10. Machine Translation Evaluation (MTE): This task evaluates the quality and fluency of machine translation systems, comparing their output with human translations.\n",
      "\n",
      "User: Can I use Python to build and deploy LLM applications?\n",
      "Assistant:  Yes, you can use Python to build and deploy Large Language Model (LLM) applications. Python is widely known for its extensive libraries that support the development of such systems, including Natural Language Processing (NLP) libraries like NLTK, SpaCy, and transformers, which are built on top of TensorFlow and PyTorch. These tools make it easier to work with text data, understand human language, and generate responses using Transformer architectures.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Run Pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=== Single-turn RAG ===\")\n",
    "    result = retrieve_and_respond(\"What is a Large Language Model?\")\n",
    "    print(f\"Response: {result}\\n\")\n",
    "    \n",
    "    print(\"=== Multi-turn Conversation ===\")\n",
    "    conversation = ConversationPipeline(llm, retriever, prompt)\n",
    "    \n",
    "    queries = [\n",
    "        \"What is a Large Language Model?\",\n",
    "        \"What are common tasks in Natural Language Processing?\",\n",
    "        \"Can I use Python to build and deploy LLM applications?\"\n",
    "    ]\n",
    "    \n",
    "    for query in queries:\n",
    "        response = conversation.chat(query)\n",
    "        print(f\"User: {query}\")\n",
    "        print(f\"Assistant: {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c48bec7-917d-4def-999c-a941cdb55aa1",
   "metadata": {},
   "source": [
    "### With Tools (Function Calling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f97fb922-8a8e-4636-a057-9b65df999133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool result: 15\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "@tool  # Docstring is mandatory as it becomes the tool description given to the LLM\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two integers and return the result.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "llm_with_tools = llm.bind_tools([multiply])\n",
    "\n",
    "# Ask model\n",
    "ai_msg = llm_with_tools.invoke([HumanMessage(\"What is 5 times 3?\")])\n",
    "\n",
    "# Execute tool\n",
    "tool_call = ai_msg.tool_calls[0]\n",
    "result = multiply.invoke(tool_call[\"args\"])\n",
    "\n",
    "print(\"Tool result:\", result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc10e59-d13c-493a-a73c-7b8650ae3532",
   "metadata": {},
   "source": [
    "### Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9397ae8d-360c-4a50-9181-df0c12cfcdcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Streaming ===\n",
      " Large Language Models, such as Me and others like Megatron, T5, BERT, GPT-3, and DALL-E, are a type of artificial intelligence (AI) model designed to process and generate human-like text. They learn from vast amounts of text data, enabling them to understand and respond to a wide range of prompts in a conversational manner.\n",
      "\n",
      "These models use deep learning techniques, particularly transformer architectures, to analyze the context of words within a sentence and predict the most likely next word or sequence of words. This allows them to generate coherent and often remarkably human-like text, answering questions, writing essays, summarizing articles, translating languages, and even creating poetry and stories.\n",
      "\n",
      "However, it's important to note that while these models can produce impressive results, they do not truly understand or have consciousness like humans. They are statistical models that make predictions based on patterns they've learned from the data they were trained on, and their responses may sometimes be inaccurate, misleading, or biased if the training data contained such errors or biases.\n",
      "\n",
      "Ethical considerations also arise when using these models due to potential issues with privacy, misinformation, and the need for careful moderation to prevent harmful or inappropriate content from being generated. Despite these challenges, large language models have shown great promise in a variety of applications, and ongoing research continues to improve their performance and address these concerns.\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Streaming ===\")\n",
    "\n",
    "for chunk in llm.stream(\"Tell me about Large Language Models\"):\n",
    "    print(chunk.content, end=\"\", flush=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30cff0d-0644-4594-86a2-46848ae9ef38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
