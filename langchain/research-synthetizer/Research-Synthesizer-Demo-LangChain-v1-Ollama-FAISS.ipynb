{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f980ae1a-1e9d-4586-b9ce-9313c9732fa1",
   "metadata": {},
   "source": [
    "# ðŸ”¬ Research Synthetizer\n",
    "\n",
    "**AI-powered research assistant that synthesizes comprehensive answers from multiple sources.**\n",
    "\n",
    "<img src=\"img/research-synthetizer.png\" alt=\"Overview\" width=\"75%\">\n",
    "\n",
    "\n",
    "## Features\n",
    "- ðŸ“‚ **Local Documents**: Index and search PDFs/TXT files via FAISS vector database\n",
    "- ðŸ“š **arXiv Papers**: Automatic academic paper retrieval and analysis\n",
    "- ðŸŒ **Web Search**: Real-time information via Tavily API (optional)\n",
    "- ðŸ¤– **Smart RAG Pipeline**: HuggingFace embeddings + cross-encoder reranking\n",
    "- ðŸ’¬ **Modern UI**: Gradio interface with collapsible sidebar and report management\n",
    "\n",
    "## Tech Stack\n",
    "- **LLM**: Ollama (Qwen, Mistral, Deepseek) via LangChain/LangGraph\n",
    "- **Vector DB**: FAISS with `all-MiniLM-L6-v2` embeddings\n",
    "- **Reranking**: Cross-encoder `ms-marco-MiniLM-L-6-v2`\n",
    "- **Interface**: Gradio 6.1+\n",
    "\n",
    "## Quick Start\n",
    "1. Place documents in `./research_docs`\n",
    "2. Configure `CONFIG` dictionary (model, API keys, retrieval settings)\n",
    "3. Run the app - it auto-indexes documents and launches web UI\n",
    "4. Ask research questions - reports saved to `./report_docs`\n",
    "\n",
    "---\n",
    "*Research tool for educational purposes. Verify important information from primary sources.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf834a58-bc52-4e62-a58f-a7988eeafeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import operator\n",
    "import json\n",
    "import hashlib\n",
    "import pickle\n",
    "import operator\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, TypedDict, Annotated, Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e539cc-1e4f-4f45-8f20-8a50d6f056cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports (LangChain v1 / current integrations)\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_community.utilities import ArxivAPIWrapper\n",
    "from langchain_community.retrievers.tavily_search_api import TavilySearchAPIRetriever\n",
    "\n",
    "# LangGraph imports\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# Reranking imports\n",
    "from sentence_transformers import CrossEncoder\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import Markdown, display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d787d2bd-b44c-4809-8661-d589df5396a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Global Configuration\n",
    "# =============================================================================\n",
    "llms = [\"qwen2.5:3b\", \"mistral:7b\", \"gpt-oss:20b\"]\n",
    "\n",
    "CONFIG: Dict[str, Any] = {\n",
    "    \"model_name\": llms[0],\n",
    "    \"embedding_model\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    \"reranker_model\": \"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n",
    "    \"tavily_api_key\" : None,\n",
    "    \"faiss_dir\": \"./vector_db\",\n",
    "    \"docs_dir\": \"./research_docs\",\n",
    "    \"reports_dir\": \"./report_docs\",\n",
    "    \"use_reranking\": True,\n",
    "    \"top_k_initial\": 30,\n",
    "    \"top_k_final\": 7,\n",
    "    \"temperature\": 0.2,\n",
    "    \"context_window\": 8192 #4096 #8192\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a1dcc1-1aed-4a5f-abe8-711249bcb1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearchState(TypedDict):\n",
    "    \"\"\"State for the research workflow\"\"\"\n",
    "    query: str\n",
    "    document_results: str\n",
    "    arxiv_results: str\n",
    "    web_results: str\n",
    "    messages: Annotated[List, operator.add]\n",
    "    final_report: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88481496-701d-4902-a11d-198057e9569b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearchSynthetizer:\n",
    "    \"\"\"Multi-source research synthetizer with RAG capabilities and persistent vector store\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = CONFIG[\"model_name\"],\n",
    "        embedding_model: str = CONFIG[\"embedding_model\"],\n",
    "        reranker_model: str = CONFIG[\"reranker_model\"],\n",
    "        tavily_api_key: str = CONFIG[\"tavily_api_key\"],\n",
    "        vector_db_path: str = CONFIG[\"faiss_dir\"],\n",
    "        docs_directory: str = CONFIG[\"docs_dir\"],\n",
    "        reports_directory: str = CONFIG[\"reports_dir\"],\n",
    "        use_reranking: bool = CONFIG[\"use_reranking\"],\n",
    "        top_k_initial: int = CONFIG[\"top_k_initial\"],\n",
    "        top_k_final: int = CONFIG[\"top_k_final\"],\n",
    "        temperature: float = CONFIG[\"temperature\"],\n",
    "        context_window: int = CONFIG[\"context_window\"], \n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the research assistant\n",
    "        \n",
    "        Args:\n",
    "            model_name: Ollama model name (e.g., 'llama3.2', 'mistral')\n",
    "            embedding_model: HuggingFace embedding model for vector search\n",
    "            reranker_model: Cross-encoder model for reranking\n",
    "            tavily_api_key: API key for Tavily search (optional)\n",
    "            vector_db_path: Path to store the persistent vector database\n",
    "            docs_directory: Directory to monitor for PDF/text files\n",
    "            use_reranking: Whether to use cross-encoder reranking\n",
    "            top_k_initial: Number of documents to retrieve initially\n",
    "            top_k_final: Number of documents to return after reranking\n",
    "        \"\"\"\n",
    "        # Initialize local LLM\n",
    "        self.llm = ChatOllama(\n",
    "            model=model_name,\n",
    "            temperature=temperature,\n",
    "            num_ctx=context_window\n",
    "        )\n",
    "        \n",
    "        # Initialize embeddings for RAG using langchain-huggingface\n",
    "        self.embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=embedding_model\n",
    "        )\n",
    "        \n",
    "        # Initialize reranker\n",
    "        self.use_reranking = use_reranking\n",
    "        self.top_k_initial = top_k_initial\n",
    "        self.top_k_final = top_k_final\n",
    "        \n",
    "        if use_reranking:\n",
    "            print(f\"Loading cross-encoder reranker: {reranker_model}\")\n",
    "            self.reranker = CrossEncoder(reranker_model)\n",
    "            print(\"âœ“ Reranker loaded successfully\")\n",
    "        else:\n",
    "            self.reranker = None\n",
    "        \n",
    "        # Setup paths\n",
    "        self.vector_db_path = Path(vector_db_path)\n",
    "        self.docs_directory = Path(docs_directory)\n",
    "        self.vector_db_path.mkdir(exist_ok=True)\n",
    "        self.docs_directory.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Tracking file\n",
    "        self.index_file = self.vector_db_path / \"indexed_files.json\"\n",
    "        self.faiss_index_path = self.vector_db_path / \"faiss_index\"\n",
    "        \n",
    "        # Vector store for documents\n",
    "        self.vectorstore = None\n",
    "        self.documents = []\n",
    "        self.indexed_files: Set[str] = self._load_indexed_files()\n",
    "        \n",
    "        # Initialize search APIs\n",
    "        self.arxiv = ArxivAPIWrapper(top_k_results=3, doc_content_chars_max=5000)\n",
    "        \n",
    "        # Initialize Tavily\n",
    "        self.tavily_api_key = tavily_api_key\n",
    "        if tavily_api_key:\n",
    "            os.environ[\"TAVILY_API_KEY\"] = tavily_api_key\n",
    "            self.tavily_retriever = TavilySearchAPIRetriever(k=3)\n",
    "        else:\n",
    "            self.tavily_retriever = None\n",
    "        \n",
    "        # Load existing vector store or initialize empty\n",
    "        self._load_or_create_vectorstore()\n",
    "        \n",
    "        # Build the workflow graph\n",
    "        self.workflow = self._build_workflow()\n",
    "    \n",
    "    def _get_file_hash(self, filepath: Path) -> str:\n",
    "        \"\"\"Generate hash of file content for change detection\"\"\"\n",
    "        hasher = hashlib.md5()\n",
    "        with open(filepath, 'rb') as f:\n",
    "            buf = f.read()\n",
    "            hasher.update(buf)\n",
    "        return hasher.hexdigest()\n",
    "    \n",
    "    def _load_indexed_files(self) -> Set[str]:\n",
    "        \"\"\"Load the set of already indexed files\"\"\"\n",
    "        if self.index_file.exists():\n",
    "            with open(self.index_file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                return set(data.get('files', []))\n",
    "        return set()\n",
    "    \n",
    "    def _save_indexed_files(self):\n",
    "        \"\"\"Save the set of indexed files\"\"\"\n",
    "        with open(self.index_file, 'w') as f:\n",
    "            json.dump({'files': list(self.indexed_files)}, f, indent=2)\n",
    "    \n",
    "    def _load_or_create_vectorstore(self):\n",
    "        \"\"\"Load existing FAISS index or create a new one\"\"\"\n",
    "        if self.faiss_index_path.exists():\n",
    "            try:\n",
    "                print(\"Loading existing vector database...\")\n",
    "                self.vectorstore = FAISS.load_local(\n",
    "                    str(self.faiss_index_path),\n",
    "                    self.embeddings,\n",
    "                    allow_dangerous_deserialization=True\n",
    "                )\n",
    "                print(f\"Loaded vector database with {self.vectorstore.index.ntotal} vectors\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading vector database: {e}\")\n",
    "                print(\"Creating new vector database...\")\n",
    "                self.vectorstore = None\n",
    "        else:\n",
    "            print(\"No existing vector database found. Will create on first document load.\")\n",
    "            self.vectorstore = None\n",
    "    \n",
    "    def _save_vectorstore(self):\n",
    "        \"\"\"Save the FAISS index to disk\"\"\"\n",
    "        if self.vectorstore:\n",
    "            self.vectorstore.save_local(str(self.faiss_index_path))\n",
    "            print(f\"Vector database saved with {self.vectorstore.index.ntotal} vectors\")\n",
    "    \n",
    "    def scan_and_load_documents(self):\n",
    "        \"\"\"\n",
    "        Scan the documents directory and load only new or modified files\n",
    "        \"\"\"\n",
    "        print(f\"\\nScanning directory: {self.docs_directory}\")\n",
    "        \n",
    "        # Find all PDF and text files\n",
    "        file_patterns = ['*.pdf', '*.txt', '*.md']\n",
    "        all_files = []\n",
    "        for pattern in file_patterns:\n",
    "            all_files.extend(self.docs_directory.glob(pattern))\n",
    "        \n",
    "        if not all_files:\n",
    "            print(\"No PDF or text files found in the directory.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"Found {len(all_files)} total files\")\n",
    "        \n",
    "        # Identify new or modified files\n",
    "        new_files = []\n",
    "        for file_path in all_files:\n",
    "            file_hash = f\"{file_path.name}:{self._get_file_hash(file_path)}\"\n",
    "            if file_hash not in self.indexed_files:\n",
    "                new_files.append(file_path)\n",
    "        \n",
    "        if not new_files:\n",
    "            print(\"No new or modified documents to process.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"Processing {len(new_files)} new/modified documents...\")\n",
    "        \n",
    "        # Load new documents\n",
    "        new_docs = []\n",
    "        processed_hashes = []\n",
    "        \n",
    "        for file_path in new_files:\n",
    "            try:\n",
    "                if file_path.suffix.lower() == '.pdf':\n",
    "                    loader = PyPDFLoader(str(file_path))\n",
    "                elif file_path.suffix.lower() in ['.txt', '.md']:\n",
    "                    loader = TextLoader(str(file_path))\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "                docs = loader.load()\n",
    "                new_docs.extend(docs)\n",
    "                file_hash = f\"{file_path.name}:{self._get_file_hash(file_path)}\"\n",
    "                processed_hashes.append(file_hash)\n",
    "                print(f\"  âœ“ Loaded: {file_path.name} ({len(docs)} pages/chunks)\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"  âœ— Error loading {file_path.name}: {e}\")\n",
    "        \n",
    "        if not new_docs:\n",
    "            print(\"No documents were successfully loaded.\")\n",
    "            return\n",
    "        \n",
    "        # Split documents\n",
    "        print(\"\\nSplitting documents into chunks...\")\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200\n",
    "        )\n",
    "        new_chunks = text_splitter.split_documents(new_docs)\n",
    "        print(f\"Created {len(new_chunks)} new chunks\")\n",
    "        \n",
    "        # Add to vector store\n",
    "        if self.vectorstore is None:\n",
    "            # Create new vector store\n",
    "            print(\"Creating new vector database...\")\n",
    "            self.vectorstore = FAISS.from_documents(new_chunks, self.embeddings)\n",
    "        else:\n",
    "            # Add to existing vector store\n",
    "            print(\"Adding to existing vector database...\")\n",
    "            self.vectorstore.add_documents(new_chunks)\n",
    "        \n",
    "        # Update indexed files\n",
    "        self.indexed_files.update(processed_hashes)\n",
    "        self._save_indexed_files()\n",
    "        self._save_vectorstore()\n",
    "        \n",
    "        print(f\"\\nâœ“ Vector database updated successfully!\")\n",
    "        print(f\"  Total vectors: {self.vectorstore.index.ntotal}\")\n",
    "        print(f\"  Total indexed files: {len(self.indexed_files)}\")\n",
    "    \n",
    "    def load_documents(self, file_paths: List[str]):\n",
    "        \"\"\"\n",
    "        Legacy method for backward compatibility\n",
    "        Loads specific files (use scan_and_load_documents for directory monitoring)\n",
    "        \"\"\"\n",
    "        all_docs = []\n",
    "        \n",
    "        for path in file_paths:\n",
    "            path_obj = Path(path)\n",
    "            \n",
    "            if not path_obj.exists():\n",
    "                print(f\"Warning: {path} does not exist, skipping...\")\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                if path_obj.suffix.lower() == '.pdf':\n",
    "                    loader = PyPDFLoader(path)\n",
    "                elif path_obj.suffix.lower() in ['.txt', '.md']:\n",
    "                    loader = TextLoader(path)\n",
    "                else:\n",
    "                    print(f\"Warning: Unsupported file type {path_obj.suffix}\")\n",
    "                    continue\n",
    "                \n",
    "                docs = loader.load()\n",
    "                all_docs.extend(docs)\n",
    "                print(f\"Loaded {len(docs)} pages/chunks from {path}\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {path}: {e}\")\n",
    "        \n",
    "        if all_docs:\n",
    "            # Split documents using langchain-text-splitters\n",
    "            text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=1000,\n",
    "                chunk_overlap=200\n",
    "            )\n",
    "            new_chunks = text_splitter.split_documents(all_docs)\n",
    "            \n",
    "            if self.vectorstore is None:\n",
    "                self.vectorstore = FAISS.from_documents(new_chunks, self.embeddings)\n",
    "            else:\n",
    "                self.vectorstore.add_documents(new_chunks)\n",
    "            \n",
    "            print(f\"Total chunks: {self.vectorstore.index.ntotal}\")\n",
    "        else:\n",
    "            print(\"No documents were loaded successfully\")\n",
    "    \n",
    "    def _rerank_documents(self, query: str, documents: List[Any]) -> List[Any]:\n",
    "        \"\"\"\n",
    "        Rerank documents using cross-encoder for better relevance\n",
    "        \n",
    "        Args:\n",
    "            query: The search query\n",
    "            documents: List of documents to rerank\n",
    "            \n",
    "        Returns:\n",
    "            Reranked list of documents\n",
    "        \"\"\"\n",
    "        if not self.reranker or not documents:\n",
    "            return documents\n",
    "        \n",
    "        # Prepare query-document pairs for cross-encoder\n",
    "        pairs = [[query, doc.page_content] for doc in documents]\n",
    "        \n",
    "        # Get relevance scores from cross-encoder\n",
    "        scores = self.reranker.predict(pairs)\n",
    "        \n",
    "        # Sort documents by score (descending)\n",
    "        scored_docs = list(zip(documents, scores))\n",
    "        scored_docs.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Return top-k documents\n",
    "        reranked_docs = [doc for doc, score in scored_docs[:self.top_k_final]]\n",
    "        \n",
    "        # Log reranking results\n",
    "        print(f\"\\n  Reranking results:\")\n",
    "        for i, (doc, score) in enumerate(scored_docs[:self.top_k_final], 1):\n",
    "            source = doc.metadata.get('source', 'Unknown')\n",
    "            print(f\"    {i}. Score: {score:.4f} - {Path(source).name}\")\n",
    "        \n",
    "        return reranked_docs\n",
    "\n",
    "    \n",
    "        \n",
    "    def _search_documents(self, state: ResearchState) -> ResearchState:\n",
    "        \"\"\"Search loaded documents using RAG with optional reranking\"\"\"\n",
    "        query = state[\"query\"]\n",
    "        \n",
    "        if not self.vectorstore:\n",
    "            state[\"document_results\"] = \"No documents loaded.\"\n",
    "            return state\n",
    "        \n",
    "        try:\n",
    "            # Retrieve more documents initially for reranking\n",
    "            k = self.top_k_initial if self.use_reranking else self.top_k_final\n",
    "            docs = self.vectorstore.similarity_search(query, k=k)\n",
    "            \n",
    "            if not docs:\n",
    "                state[\"document_results\"] = \"No relevant information found in loaded documents.\"\n",
    "                return state\n",
    "            \n",
    "            print(f\"  Initial retrieval: {len(docs)} documents\")\n",
    "            \n",
    "            # Apply reranking if enabled\n",
    "            if self.use_reranking:\n",
    "                docs = self._rerank_documents(query, docs)\n",
    "                print(f\"  After reranking: {len(docs)} documents\")\n",
    "            \n",
    "            results = []\n",
    "            for i, doc in enumerate(docs, 1):\n",
    "                source = doc.metadata.get('source', 'Unknown')\n",
    "                page = doc.metadata.get('page', 'N/A')\n",
    "                results.append(\n",
    "                    f\"[Document {i} - {Path(source).name}, Page {page}]\\n\"\n",
    "                    f\"{doc.page_content[:500]}...\"\n",
    "                )\n",
    "            \n",
    "            state[\"document_results\"] = \"\\n\\n\".join(results)\n",
    "            state[\"messages\"].append(\n",
    "                AIMessage(content=f\"Found {len(docs)} relevant document chunks (with reranking).\")\n",
    "            )\n",
    "        \n",
    "        except Exception as e:\n",
    "            state[\"document_results\"] = f\"Error searching documents: {e}\"\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def _search_arxiv(self, state: ResearchState) -> ResearchState:\n",
    "        \"\"\"Search ArXiv for research papers\"\"\"\n",
    "        query = state[\"query\"]\n",
    "        \n",
    "        try:\n",
    "            results = self.arxiv.run(query)\n",
    "            state[\"arxiv_results\"] = results if results else \"No papers found on ArXiv.\"\n",
    "            state[\"messages\"].append(\n",
    "                AIMessage(content=\"Searched ArXiv for relevant papers.\")\n",
    "            )\n",
    "        except Exception as e:\n",
    "            state[\"arxiv_results\"] = f\"Error searching ArXiv: {e}\"\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def _search_web(self, state: ResearchState) -> ResearchState:\n",
    "        \"\"\"Search the web using Tavily\"\"\"\n",
    "        if not self.tavily_retriever:\n",
    "            state[\"web_results\"] = \"Tavily search not configured.\"\n",
    "            return state\n",
    "        \n",
    "        query = state[\"query\"]\n",
    "        \n",
    "        try:\n",
    "            docs = self.tavily_retriever.invoke(query)\n",
    "            if not docs:\n",
    "                state[\"web_results\"] = \"No web results found.\"\n",
    "                return state\n",
    "            \n",
    "            results = []\n",
    "            for i, doc in enumerate(docs, 1):\n",
    "                content = doc.page_content[:500]\n",
    "                source = doc.metadata.get('source', 'Unknown')\n",
    "                results.append(f\"[Result {i} - {source}]\\n{content}...\")\n",
    "            \n",
    "            state[\"web_results\"] = \"\\n\\n\".join(results)\n",
    "            state[\"messages\"].append(\n",
    "                AIMessage(content=f\"Found {len(docs)} web results.\")\n",
    "            )\n",
    "        except Exception as e:\n",
    "            state[\"web_results\"] = f\"Error searching web: {e}\"\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def _synthesize_results(self, state: ResearchState) -> ResearchState:\n",
    "        \"\"\"Synthesize all research findings into a comprehensive report\"\"\"\n",
    "        query = state[\"query\"]\n",
    "        doc_results = state.get(\"document_results\", \"\")\n",
    "        arxiv_results = state.get(\"arxiv_results\", \"\")\n",
    "        web_results = state.get(\"web_results\", \"\")\n",
    "        \n",
    "        # Create synthesis prompt\n",
    "        synthesis_prompt = f\"\"\"You are a research synthesizer. Your task is to analyze and combine information from multiple sources into a comprehensive, well-structured report.\n",
    "\n",
    "Research Query: {query}\n",
    "\n",
    "--- DOCUMENT SEARCH RESULTS ---\n",
    "{doc_results}\n",
    "\n",
    "--- ARXIV RESEARCH PAPERS ---\n",
    "{arxiv_results}\n",
    "\n",
    "--- WEB SEARCH RESULTS ---\n",
    "{web_results}\n",
    "\n",
    "Please synthesize the above information into a comprehensive research report that:\n",
    "1. Provides a clear overview of the topic\n",
    "2. Identifies key findings from each source\n",
    "3. Notes any patterns, contradictions, or gaps\n",
    "4. Offers actionable insights and conclusions\n",
    "5. Cites sources appropriately\n",
    "6. When tools return source URLs, cite them as markdown links: [Source Name](URL)\n",
    "7. ONLY include links that are actually provided by the tools - never create or guess URLs\n",
    "\n",
    "Generate your synthesis now:\"\"\"\n",
    "\n",
    "        try:\n",
    "            messages = [\n",
    "                SystemMessage(content=\"You are an expert research analyst skilled at synthesizing information from multiple sources.\"),\n",
    "                HumanMessage(content=synthesis_prompt)\n",
    "            ]\n",
    "            \n",
    "            response = self.llm.invoke(messages)\n",
    "            state[\"final_report\"] = response.content\n",
    "            state[\"messages\"].append(\n",
    "                AIMessage(content=\"Research synthesis completed.\")\n",
    "            )\n",
    "        except Exception as e:\n",
    "            state[\"final_report\"] = f\"Error during synthesis: {e}\"\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def _build_workflow(self) -> StateGraph:\n",
    "        \"\"\"Build the research workflow graph\"\"\"\n",
    "        workflow = StateGraph(ResearchState)\n",
    "        \n",
    "        # Add nodes\n",
    "        workflow.add_node(\"search_documents\", self._search_documents)\n",
    "        workflow.add_node(\"search_arxiv\", self._search_arxiv)\n",
    "        workflow.add_node(\"search_web\", self._search_web)\n",
    "        workflow.add_node(\"synthesize\", self._synthesize_results)\n",
    "        \n",
    "        # Define the flow\n",
    "        workflow.set_entry_point(\"search_documents\")\n",
    "        workflow.add_edge(\"search_documents\", \"search_arxiv\")\n",
    "        workflow.add_edge(\"search_arxiv\", \"search_web\")\n",
    "        workflow.add_edge(\"search_web\", \"synthesize\")\n",
    "        workflow.add_edge(\"synthesize\", END)\n",
    "        \n",
    "        return workflow.compile()\n",
    "    \n",
    "    def research(self, query: str, use_docs: bool = True, \n",
    "                 use_arxiv: bool = True, use_web: bool = False) -> str:\n",
    "        \"\"\"\n",
    "        Conduct comprehensive research on a query\n",
    "        \n",
    "        Args:\n",
    "            query: Research question or topic\n",
    "            use_docs: Search uploaded documents\n",
    "            use_arxiv: Search ArXiv papers\n",
    "            use_web: Search the web (requires Tavily API key)\n",
    "        \n",
    "        Returns:\n",
    "            Comprehensive research report\n",
    "        \"\"\"\n",
    "        # Initialize state\n",
    "        initial_state = {\n",
    "            \"query\": query,\n",
    "            \"document_results\": \"\",\n",
    "            \"arxiv_results\": \"\",\n",
    "            \"web_results\": \"\",\n",
    "            \"messages\": [HumanMessage(content=f\"Research query: {query}\")],\n",
    "            \"final_report\": \"\"\n",
    "        }\n",
    "        \n",
    "        # Build custom workflow based on enabled sources\n",
    "        workflow = StateGraph(ResearchState)\n",
    "        \n",
    "        nodes_to_add = []\n",
    "        \n",
    "        if use_docs and self.vectorstore:\n",
    "            workflow.add_node(\"search_documents\", self._search_documents)\n",
    "            nodes_to_add.append(\"search_documents\")\n",
    "        \n",
    "        if use_arxiv:\n",
    "            workflow.add_node(\"search_arxiv\", self._search_arxiv)\n",
    "            nodes_to_add.append(\"search_arxiv\")\n",
    "        \n",
    "        if use_web and self.tavily_retriever:\n",
    "            workflow.add_node(\"search_web\", self._search_web)\n",
    "            nodes_to_add.append(\"search_web\")\n",
    "        \n",
    "        if not nodes_to_add:\n",
    "            return \"No research sources enabled or available. Please enable at least one source.\"\n",
    "        \n",
    "        workflow.add_node(\"synthesize\", self._synthesize_results)\n",
    "        nodes_to_add.append(\"synthesize\")\n",
    "        \n",
    "        # Connect nodes sequentially\n",
    "        workflow.set_entry_point(nodes_to_add[0])\n",
    "        for i in range(len(nodes_to_add) - 1):\n",
    "            workflow.add_edge(nodes_to_add[i], nodes_to_add[i + 1])\n",
    "        workflow.add_edge(nodes_to_add[-1], END)\n",
    "        \n",
    "        # Compile and run\n",
    "        app = workflow.compile()\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 75)\n",
    "        print(\"RUNNING RESEARCH WORKFLOW\")\n",
    "        print(\"=\" * 75)\n",
    "        \n",
    "        result = app.invoke(initial_state)\n",
    "        \n",
    "        return result[\"final_report\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da77b474-8297-4d85-84bf-4987740ec513",
   "metadata": {},
   "source": [
    "## Gradio UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907f912c-1d9b-4173-aa8d-53a8ceb85ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import string\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# -------------------------\n",
    "# Assistant logic\n",
    "# -------------------------\n",
    "assistant = None\n",
    "\n",
    "def initialize_assistant():\n",
    "    global assistant\n",
    "    if assistant is None:\n",
    "        assistant = ResearchSynthetizer(\n",
    "            model_name=CONFIG[\"model_name\"],\n",
    "            embedding_model=CONFIG[\"embedding_model\"],\n",
    "            reranker_model=CONFIG[\"reranker_model\"],\n",
    "            tavily_api_key=CONFIG[\"tavily_api_key\"],\n",
    "            vector_db_path=CONFIG[\"faiss_dir\"],\n",
    "            docs_directory=CONFIG[\"docs_dir\"],\n",
    "            reports_directory=CONFIG[\"reports_dir\"],\n",
    "            use_reranking=CONFIG[\"use_reranking\"],\n",
    "            top_k_initial=CONFIG[\"top_k_initial\"],\n",
    "            top_k_final=CONFIG[\"top_k_final\"],\n",
    "            temperature=CONFIG[\"temperature\"],\n",
    "            context_window=CONFIG[\"context_window\"],\n",
    "        )\n",
    "        assistant.scan_and_load_documents()\n",
    "    return assistant\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Reports utilities\n",
    "# -------------------------\n",
    "def get_saved_reports():\n",
    "    \"\"\"Get saved reports and format them for display\"\"\"\n",
    "    if not os.path.exists(CONFIG[\"reports_dir\"]):\n",
    "        return []\n",
    "    files = sorted(\n",
    "        [\n",
    "            f for f in os.listdir(CONFIG[\"reports_dir\"])\n",
    "            if f.endswith(\".md\")\n",
    "        ],\n",
    "        reverse=True\n",
    "    )\n",
    "    # Return list of tuples: (display_name, actual_filename)\n",
    "    formatted = []\n",
    "    for f in files:\n",
    "        # Remove .md extension and replace underscores with spaces\n",
    "        display_name = f.replace(\".md\", \"\").replace(\"_\", \" \")\n",
    "        # Remove timestamp pattern if present\n",
    "        display_name = re.sub(r'\\s+CoT\\s+\\d{8}\\s+\\d{6}$', '', display_name)\n",
    "        formatted.append((display_name, f))\n",
    "    return formatted\n",
    "\n",
    "\n",
    "def extract_title_from_markdown(content):\n",
    "    \"\"\"Extract title from markdown file (first # heading)\"\"\"\n",
    "    # Look for first markdown heading (# Title)\n",
    "    match = re.search(r'^#\\s+(.+?)$', content, re.MULTILINE)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    \n",
    "    # Fallback: try to get first non-empty line\n",
    "    lines = content.strip().split('\\n')\n",
    "    for line in lines:\n",
    "        clean_line = line.strip().lstrip('#').strip()\n",
    "        if clean_line:\n",
    "            return clean_line[:100]  # Limit to 100 chars\n",
    "    \n",
    "    return \"Loaded Document\"\n",
    "\n",
    "\n",
    "def load_report(filename, history):\n",
    "    \"\"\"Load a saved report, clear chat first, extract title, and display\"\"\"\n",
    "    if not filename:\n",
    "        return history, \"ðŸ” **Active Research:** *No query yet*\", gr.update()\n",
    "\n",
    "    filepath = os.path.join(CONFIG[\"reports_dir\"], filename)\n",
    "    if not os.path.exists(filepath):\n",
    "        return history, \"ðŸ” **Active Research:** *No query yet*\", gr.update()\n",
    "\n",
    "    # Read the report content\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read()\n",
    "\n",
    "    # Extract title from markdown\n",
    "    title = extract_title_from_markdown(content)\n",
    "    \n",
    "    # Clear history and add the loaded report\n",
    "    new_history = [{\"role\": \"assistant\", \"content\": content}]\n",
    "    \n",
    "    # Update topic display\n",
    "    topic_text = f\"ðŸ“„ **Loaded Report:** {title}\"\n",
    "    \n",
    "    # Update radio button selection\n",
    "    return new_history, topic_text, gr.update(value=filename)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Research handler\n",
    "# -------------------------\n",
    "def research_query(query, history):\n",
    "    if not query.strip():\n",
    "        return (\n",
    "            history + [{\"role\": \"assistant\", \"content\": \"Please enter a research question.\"}],\n",
    "            \"ðŸ” **Active Research:** *No query yet*\",\n",
    "            gr.update(choices=get_saved_reports()),\n",
    "        )\n",
    "\n",
    "    assistant = initialize_assistant()\n",
    "\n",
    "    history = history + [{\"role\": \"user\", \"content\": query}]\n",
    "    history = history + [{\"role\": \"assistant\", \"content\": f\"ðŸ§  **Researching:** {query}\"}]\n",
    "\n",
    "    yield (\n",
    "        history,\n",
    "        f\"ðŸ” **Active Research:** {query}\",\n",
    "        gr.update(choices=get_saved_reports()),\n",
    "    )\n",
    "\n",
    "    result = assistant.research(\n",
    "        query=query,\n",
    "        use_docs=True,\n",
    "        use_arxiv=True,\n",
    "        use_web=True,\n",
    "    )\n",
    "\n",
    "    # Save result\n",
    "    no_punct = query.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    title = re.sub(r\"\\s+\", \"_\", no_punct)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"{title}_CoT_{timestamp}.md\"\n",
    "    filepath = os.path.join(CONFIG[\"reports_dir\"], filename)\n",
    "\n",
    "    os.makedirs(CONFIG[\"reports_dir\"], exist_ok=True)\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(result)\n",
    "\n",
    "    history[-1] = {\"role\": \"assistant\", \"content\": result}\n",
    "\n",
    "    yield (\n",
    "        history,\n",
    "        f\"ðŸ” **Active Research:** {query}\",\n",
    "        gr.update(choices=get_saved_reports(), value=filename),\n",
    "    )\n",
    "\n",
    "\n",
    "def clear_chat():\n",
    "    \"\"\"Clear chat and reset topic\"\"\"\n",
    "    return [], \"ðŸ” **Active Research:** *No query yet*\", gr.update(value=None)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# UI\n",
    "# -------------------------\n",
    "with gr.Blocks(title=\"Research Synthetizer\") as demo:\n",
    "\n",
    "    with gr.Row(elem_id=\"app-grid\"):\n",
    "\n",
    "        # SIDEBAR\n",
    "        with gr.Column(elem_id=\"sidebar\"):\n",
    "            toggle_btn = gr.Button(\"â—€\", elem_id=\"sidebar-toggle\")\n",
    "            \n",
    "            gr.Markdown(\"## ðŸ”¬ Research Synthetizer\")\n",
    "\n",
    "            gr.Markdown(\"\"\"\n",
    "            **Data Sources**\n",
    "            - ðŸ“‚ Local documents  \n",
    "            - ðŸ“š arXiv papers  \n",
    "            - ðŸŒ Web sources\n",
    "            \"\"\")\n",
    "\n",
    "            clear_btn = gr.Button(\"ðŸ—‘ Clear Chat\")\n",
    "            \n",
    "            # SAVED REPORTS IN SIDEBAR\n",
    "            gr.Markdown(\"---\")\n",
    "            gr.Markdown(\"### ðŸ“‚ Saved Reports\")\n",
    "            reports_radio = gr.Radio(\n",
    "                choices=get_saved_reports(),\n",
    "                interactive=True,\n",
    "                show_label=False,\n",
    "                elem_id=\"reports-list\",\n",
    "                container=True\n",
    "            )\n",
    "\n",
    "        # MAIN\n",
    "        with gr.Column(elem_id=\"main\"):\n",
    "            topic = gr.Markdown(\"ðŸ” **Active Research:** *No query yet*\")\n",
    "            chatbot = gr.Chatbot(elem_id=\"main-chatbot\")\n",
    "            \n",
    "            with gr.Row(elem_id=\"input-row\"):\n",
    "                query = gr.Textbox(\n",
    "                    placeholder=\"Enter your research questionâ€¦\",\n",
    "                    show_label=False,\n",
    "                    container=False,\n",
    "                    scale=4\n",
    "                )\n",
    "                submit = gr.Button(\"ðŸ” Research\", elem_id=\"research-btn\", scale=1)\n",
    "\n",
    "    # -------------------------\n",
    "    # EVENTS\n",
    "    # -------------------------\n",
    "    toggle_btn.click(fn=None, inputs=None, outputs=None, js=\"toggleSidebar()\")\n",
    "\n",
    "    submit.click(\n",
    "        research_query,\n",
    "        inputs=[query, chatbot],\n",
    "        outputs=[chatbot, topic, reports_radio],\n",
    "    ).then(\n",
    "        lambda: \"\",  # Clear query input after submission\n",
    "        outputs=[query]\n",
    "    )\n",
    "\n",
    "    query.submit(\n",
    "        research_query,\n",
    "        inputs=[query, chatbot],\n",
    "        outputs=[chatbot, topic, reports_radio],\n",
    "    ).then(\n",
    "        lambda: \"\",  # Clear query input after submission\n",
    "        outputs=[query]\n",
    "    )\n",
    "\n",
    "    reports_radio.change(\n",
    "        fn=load_report,\n",
    "        inputs=[reports_radio, chatbot],\n",
    "        outputs=[chatbot, topic, reports_radio],\n",
    "    ).then(\n",
    "        fn=None,\n",
    "        inputs=None,\n",
    "        outputs=None,\n",
    "        js=\"\"\"() => {\n",
    "            setTimeout(() => {\n",
    "                const chatContainer = document.querySelector('#main-chatbot');\n",
    "                if (chatContainer) {\n",
    "                    const scrollableDiv = chatContainer.querySelector('[class*=\"overflow-y-auto\"]');\n",
    "                    if (scrollableDiv) {\n",
    "                        scrollableDiv.scrollTop = 0;\n",
    "                    }\n",
    "                }\n",
    "            }, 200);\n",
    "        }\"\"\"\n",
    "    )\n",
    "\n",
    "    clear_btn.click(\n",
    "        clear_chat,\n",
    "        outputs=[chatbot, topic, reports_radio]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df98eb8f-6788-4979-a4ce-4f7bd669ab0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Launch\n",
    "# -------------------------\n",
    "with open(\"assets/style.css\", \"r\") as f:\n",
    "    custom_css = f.read()\n",
    "\n",
    "demo.launch(\n",
    "    server_name=\"127.0.0.1\",\n",
    "    server_port=None,\n",
    "    share=False,\n",
    "    theme=gr.themes.Glass(),\n",
    "    css=custom_css,\n",
    "    js=\"\"\"\n",
    "    function toggleSidebar() {\n",
    "        const grid = document.getElementById(\"app-grid\");\n",
    "        const btn = document.getElementById(\"sidebar-toggle\");\n",
    "        grid.classList.toggle(\"collapsed\");\n",
    "        \n",
    "        if (grid.classList.contains(\"collapsed\")) {\n",
    "            btn.textContent = \"â–¶\";\n",
    "        } else {\n",
    "            btn.textContent = \"â—€\";\n",
    "        }\n",
    "    }\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04af716f-a6fa-4c2e-83f0-28b45d3f58c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fb70ca-4c9e-41ef-95ce-63ebee7af4ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf9ba00-d778-4d50-a0cb-d8ca7642774c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
