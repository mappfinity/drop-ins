{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b84a3af-4b58-46f5-9ca2-78b1a0e8a776",
   "metadata": {},
   "source": [
    "# ðŸ¤–âœ¨ Chatbot Assistant with Persistent Memory\n",
    "\n",
    "**Local, tool-enabled conversational AI using LangChain v1.0+, Ollama, and Gradio v6.1.0**\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "Fully local, production-ready chatbot with persistent memory and tool-calling capabilities. Built for open-source LLMs, privacy-sensitive environments, and offline deployments.\n",
    "\n",
    "---\n",
    "\n",
    "## Architecture\n",
    "\n",
    "### ðŸ§  Memory Management\n",
    "- **Checkpointer**: `MemorySaver` from LangGraph stores conversation state\n",
    "- **Thread-based**: Isolated `thread_id` per session for conversation history\n",
    "- **Automatic**: Memory injected/saved on each agent invocation\n",
    "\n",
    "### ðŸ”§ Agent & Tools\n",
    "- **Agent**: `create_agent()` from LangChain v1.0+ with native tool support\n",
    "- **Tools**: Wikipedia, Tavily search, Weather, and custom tools\n",
    "- **System Prompt**: Configurable personality\n",
    "\n",
    "### ðŸ¤– Local LLM (Ollama)\n",
    "- **Provider**: `ChatOllama` connects to local Ollama server\n",
    "- **Model**: Configurable (default: `qwen2.5:3b`)\n",
    "- **Temperature**: `0.0` for deterministic responses\n",
    "\n",
    "### ðŸŽ¨ UI (Gradio)\n",
    "- **Layout**: `gr.Blocks` for flexible components\n",
    "- **State**: Session-based with `gr.State`\n",
    "- **Features**: Chat history, message counter, clear button, examples\n",
    "\n",
    "---\n",
    "\n",
    "## Key Features\n",
    "\n",
    "âœ… **Persistent Memory** - Full conversation context maintained  \n",
    "âœ… **Tool Calling** - Wikipedia, Tavily web search, custom tools  \n",
    "âœ… **Local Execution** - No mandatory API keys needed or cloud dependencies  \n",
    "âœ… **Session Isolation** - Independent memory per session  \n",
    "âœ… **Error Handling** - Graceful failures with logging  \n",
    "âœ… **Modular Design** - Clean separation of concerns\n",
    "\n",
    "---\n",
    "\n",
    "## How It Works\n",
    "\n",
    "```python\n",
    "# Initialize with checkpointer for memory\n",
    "agent = create_agent(\n",
    "    model=ChatOllama(...),\n",
    "    tools=[wikipedia_tool, tavily_search, ...],\n",
    "    system_prompt=\"You are helpful...\",\n",
    "    checkpointer=MemorySaver()\n",
    ")\n",
    "\n",
    "# Invoke with thread_id for conversation history\n",
    "result = agent.invoke(\n",
    "    {\"messages\": [HumanMessage(content=user_input)]},\n",
    "    config={\"configurable\": {\"thread_id\": session_id}}\n",
    ")\n",
    "```\n",
    "\n",
    "The `MemorySaver` checkpointer loads previous messages, saves new messages, and provides full context to the agent.\n",
    "\n",
    "---\n",
    "\n",
    "## Installation\n",
    "\n",
    "```bash\n",
    "pip install langchain langchain-ollama langchain-core langgraph gradio langchain-community\n",
    "\n",
    "# Start Ollama and pull model\n",
    "ollama serve\n",
    "ollama pull qwen2.5:3b\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Usage\n",
    "\n",
    "```python\n",
    "from langchain_community.tools import WikipediaQueryRun, TavilySearchResults\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "\n",
    "# Define tools\n",
    "tools = [\n",
    "    WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper()),\n",
    "    TavilySearchResults(max_results=3)\n",
    "]\n",
    "\n",
    "# Create and launch chatbot\n",
    "bot = chatbot_demo(tools=tools, model_name=\"qwen2.5:3b\")\n",
    "demo = bot.create_ui()\n",
    "demo.launch()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Design Decisions\n",
    "\n",
    "| Aspect | Choice | Rationale |\n",
    "|--------|--------|-----------|\n",
    "| **Memory** | `MemorySaver` checkpointer | Modern LangChain v1.0+ pattern |\n",
    "| **Agent** | `create_agent()` | Replaces deprecated `create_react_agent` |\n",
    "| **Config Key** | `thread_id` | LangGraph standard for threads |\n",
    "| **LLM Provider** | Ollama | Privacy, offline, cost-free |\n",
    "| **UI Framework** | Gradio | Rapid prototyping, built-in chat |\n",
    "\n",
    "---\n",
    "\n",
    "## Extending the System\n",
    "\n",
    "### Add Custom Tools\n",
    "```python\n",
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def calculator(expression: str) -> str:\n",
    "    \"\"\"Evaluate mathematical expressions\"\"\"\n",
    "    return str(eval(expression))\n",
    "\n",
    "tools = [calculator, tavily_search, ...]\n",
    "```\n",
    "\n",
    "### Change Model\n",
    "```python\n",
    "bot = chatbot_demo(tools=tools, model_name=\"llama2:13b\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Use Cases\n",
    "\n",
    "- ðŸ  **Privacy-first applications** - All data stays local\n",
    "- ðŸ” **Research assistants** - Wikipedia + Tavily web search\n",
    "- ðŸ§ª **Experimentation** - Flexible tool integration\n",
    "- ðŸš€ **Production prototypes** - Scalable foundation\n",
    "\n",
    "---\n",
    "\n",
    "## Technical Notes\n",
    "\n",
    "- **Message Replay**: Checkpointer handles history automatically\n",
    "- **State Separation**: UI state (Gradio) â‰  Agent state (LangGraph)\n",
    "- **Thread Safety**: Each `thread_id` is isolated for concurrent sessions\n",
    "- **Tool Integration**: Tavily enables real-time web search capabilities\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "Modern LangChain implementation demonstrating:\n",
    "- âœ¨ Clean agent architecture with `create_agent()`\n",
    "- ðŸ’¾ Persistent memory via `MemorySaver`\n",
    "- ðŸ”§ Tool-augmented reasoning (Wikipedia, Tavily search)\n",
    "- ðŸŽ¯ Production-ready error handling\n",
    "\n",
    "Perfect foundation for local, tool-enabled chatbots with memory.\n",
    "\n",
    "*Based on the final project from the course **â€œFunctions, Tools and Agents with LangChainâ€** by deeplearning(dot)ai* fully updated to LangChain v1+, with a new custom UI based on Gradio and a local LLM running via Ollama."
   ]
  },
  {
   "cell_type": "raw",
   "id": "4f100a7f-de23-4519-8bc2-069fd9b1c8e1",
   "metadata": {},
   "source": [
    "%pip install -qU langchain langchain-core langchain-community langchain-ollama wikipedia-api requests pydantic gradio param tavily-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7015e8f7-3e99-4773-8aab-1ab3e81af246",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import param\n",
    "import uuid\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.agents import create_agent\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ee6b76-2e7e-41ca-8cc9-d5a45bbbad81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class chatbot_demo(param.Parameterized):\n",
    "    \"\"\"\n",
    "    Chatbot Assistant with Persistent Conversational Memory.\n",
    "\n",
    "    Memory is implemented using LangGraph's MemorySaver checkpointer.\n",
    "    Each UI session gets an isolated thread for conversation history.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tools, model_name=\"qwen2.5:3b\", **params):\n",
    "        super(chatbot_demo, self).__init__(**params)\n",
    "        self.tools = tools\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # CHECKPOINTER FOR MEMORY\n",
    "        # ------------------------------------------------------------\n",
    "        # MemorySaver stores conversation state in memory\n",
    "        self.checkpointer = MemorySaver()\n",
    "\n",
    "        # Track session IDs (thread IDs in LangGraph terminology)\n",
    "        self._session_ids = set()\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # SYSTEM PROMPT\n",
    "        # ------------------------------------------------------------\n",
    "        self.system_prompt = (\n",
    "            \"You are a helpful and friendly assistant with access to:\\n\"\n",
    "            \"- Weather Search (current conditions & forecasts)\\n\"\n",
    "            \"- Wikipedia (factual information)\\n\"\n",
    "            \"- Tavily (web search)(if available)\\n\"\n",
    "            \"\\n\"\n",
    "            \"When tools return source URLs, cite them as markdown links: [Source Name](URL)\\n\"\n",
    "            \"ONLY include links that are actually provided by the tools - never create or guess URLs.\\n\"\n",
    "            \"Use conversation context to provide relevant, personalized responses.\"\n",
    "        )\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # MODEL\n",
    "        # ------------------------------------------------------------\n",
    "        self.model = ChatOllama(\n",
    "            model=model_name,\n",
    "            temperature=0,  # increase for less deterministic results\n",
    "            base_url=\"http://localhost:11434\"\n",
    "        )\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # CREATE AGENT WITH MEMORY (via checkpointer)\n",
    "        # ------------------------------------------------------------\n",
    "        self.agent = create_agent(\n",
    "            model=self.model,\n",
    "            tools=self.tools,\n",
    "            system_prompt=self.system_prompt,\n",
    "            checkpointer=self.checkpointer  # This enables memory!\n",
    "        )\n",
    "\n",
    "    def process_chain(self, message, session_id):\n",
    "        \"\"\"\n",
    "        Process user message and return response with session-wise memory.\n",
    "\n",
    "        Args:\n",
    "            message: Current user message\n",
    "            session_id: Unique session identifier (maps to thread_id)\n",
    "\n",
    "        Returns:\n",
    "            Bot response string\n",
    "        \"\"\"\n",
    "        if not message:\n",
    "            return \"\"\n",
    "\n",
    "        try:\n",
    "            # Track this session\n",
    "            self._session_ids.add(session_id)\n",
    "\n",
    "            # Invoke agent with thread_id for memory\n",
    "            # The checkpointer automatically handles conversation history\n",
    "            result = self.agent.invoke(\n",
    "                {\n",
    "                    \"messages\": [HumanMessage(content=message)]\n",
    "                },\n",
    "                config={\n",
    "                    \"configurable\": {\n",
    "                        \"thread_id\": session_id  # thread_id is the key for memory\n",
    "                    }\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # Extract bot response (last AI message)\n",
    "            for msg in reversed(result[\"messages\"]):\n",
    "                if isinstance(msg, AIMessage):\n",
    "                    return msg.content\n",
    "            \n",
    "            # Fallback\n",
    "            return result[\"messages\"][-1].content\n",
    "\n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            error_details = traceback.format_exc()\n",
    "            print(f\"Error details:\\n{error_details}\")\n",
    "            return f\"Sorry, I encountered an error: {str(e)}\"\n",
    "\n",
    "    def clear_history(self, session_id):\n",
    "        \"\"\"Clear conversation memory for a session\"\"\"\n",
    "        if session_id:\n",
    "            # To clear memory, we need to get the checkpoint and clear it\n",
    "            # For MemorySaver, we can't directly clear, but we can track cleared sessions\n",
    "            # and create a new thread_id to effectively \"clear\" the conversation\n",
    "            pass\n",
    "        return None\n",
    "\n",
    "    def get_history_summary(self, session_id):\n",
    "        \"\"\"Get a summary of conversation history for a session\"\"\"\n",
    "        if not session_id or session_id not in self._session_ids:\n",
    "            return \"Messages in memory: 0\"\n",
    "        \n",
    "        try:\n",
    "            # Try to get state to count messages\n",
    "            config = {\"configurable\": {\"thread_id\": session_id}}\n",
    "            state = self.agent.get_state(config)\n",
    "            if state and \"messages\" in state.values:\n",
    "                msg_count = len(state.values[\"messages\"])\n",
    "                return f\"Messages in memory: {msg_count}\"\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        return \"Messages in memory: N/A\"\n",
    "\n",
    "    def create_ui(self):\n",
    "        \"\"\"Create and return Gradio ChatInterface\"\"\"\n",
    "\n",
    "        with gr.Blocks() as demo:\n",
    "            gr.Markdown(\"# ðŸ¤–âœ¨ Chatbot Assistant with Persistent Memory\")\n",
    "            gr.Markdown(\"Ask me anything! I'll remember our conversation per session.\")\n",
    "\n",
    "            chatbot = gr.Chatbot(height=500)\n",
    "\n",
    "            msg = gr.Textbox(\n",
    "                placeholder=\"Type your message here...\",\n",
    "                label=\"Message\",\n",
    "                lines=2\n",
    "            )\n",
    "\n",
    "            # Session state (one per browser session)\n",
    "            session_state = gr.State(value=None)\n",
    "\n",
    "            with gr.Row():\n",
    "                submit = gr.Button(\"Send\", variant=\"primary\")\n",
    "                clear = gr.Button(\"Clear History\")\n",
    "\n",
    "            # History info display\n",
    "            history_info = gr.Markdown(\"Messages in memory: 0\")\n",
    "\n",
    "            gr.Examples(\n",
    "                examples=[\n",
    "                    \"Latest trends in autonomous AI agents and tool use\",\n",
    "                    \"Warsaw weather (52.2297, 21.0122) - should I bring a jacket?\",\n",
    "                    \"Recent breakthroughs in AI reasoning and planning\",\n",
    "                    \"Search Wikipedia for startup accelerator programs\",\n",
    "                    \"Weather in SF (37.7749, -122.4194) for client meeting\",\n",
    "                    \"Find recent news about LangChain developments\",\n",
    "                    \"What makes an AI system 'agentic'?\",\n",
    "                    \"Recap our earlier conversation about startups\",\n",
    "                    \"Explain tool-calling in modern AI systems\",\n",
    "                    \"Explain the architecture of retrieval-augmented generation\",\n",
    "                    \"What's the current state of multimodal AI models?\",\n",
    "                    \"Weather in Paris, France for tomorrow's conference?\",\n",
    "                    \"Find current news about breakthrough in quantum computing\",\n",
    "                    \"Create a summary of all topics we've discussed today\",\n",
    "                    \"Search web for today's major tech announcements\",\n",
    "                    \"Recent advances in reinforcement learning from human feedback\"\n",
    "                ],\n",
    "                inputs=msg\n",
    "            )\n",
    "\n",
    "            def respond(message, chat_history, session_id):\n",
    "                if not message:\n",
    "                    return \"\", chat_history, session_id, history_info.value\n",
    "\n",
    "                # Create session id once per browser session\n",
    "                if session_id is None:\n",
    "                    session_id = str(uuid.uuid4())\n",
    "\n",
    "                if chat_history is None:\n",
    "                    chat_history = []\n",
    "\n",
    "                bot_message = self.process_chain(message, session_id)\n",
    "\n",
    "                # Update UI chat history\n",
    "                chat_history = chat_history + [\n",
    "                    {\"role\": \"user\", \"content\": message},\n",
    "                    {\"role\": \"assistant\", \"content\": bot_message}\n",
    "                ]\n",
    "\n",
    "                return (\n",
    "                    \"\",\n",
    "                    chat_history,\n",
    "                    session_id,\n",
    "                    self.get_history_summary(session_id)\n",
    "                )\n",
    "\n",
    "            def clear_all(chat_history, session_id):\n",
    "                # To truly clear history with MemorySaver, create a new session\n",
    "                new_session_id = str(uuid.uuid4()) if session_id else None\n",
    "                return [], new_session_id, \"Messages in memory: 0\"\n",
    "\n",
    "            # Event handlers\n",
    "            submit.click(\n",
    "                respond,\n",
    "                [msg, chatbot, session_state],\n",
    "                [msg, chatbot, session_state, history_info]\n",
    "            )\n",
    "            msg.submit(\n",
    "                respond,\n",
    "                [msg, chatbot, session_state],\n",
    "                [msg, chatbot, session_state, history_info]\n",
    "            )\n",
    "            clear.click(\n",
    "                clear_all,\n",
    "                [chatbot, session_state],\n",
    "                [chatbot, session_state, history_info]\n",
    "            )\n",
    "\n",
    "        return demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e40bbfc-61b4-441c-bf83-b429447fd387",
   "metadata": {},
   "source": [
    "### Tools available to the chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b80ec8-5f65-4cbc-a029-646260da60e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "# Create your custom tool\n",
    "@tool\n",
    "def create_your_tool(query: str) -> str:\n",
    "    \"\"\"This function can do whatever you would like once you fill it in \"\"\"\n",
    "    print(type(query))\n",
    "    return query[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fcbe19-852c-4acd-97f7-634db3a7725f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import existing tools\n",
    "from tools.web_retrievers import search_wikipedia, retrieve_tavily\n",
    "from tools.meteo import get_weather_conditions\n",
    "\n",
    "tools = [retrieve_tavily, search_wikipedia, get_weather_conditions]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11465554-a49f-42fa-8d01-d09d2f73a75b",
   "metadata": {},
   "source": [
    "### User Interface (Gradio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd89840b-9add-4041-a20d-03a998384f81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# Run the Chatbot UI \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Create agent with tools\n",
    "    chatbot = chatbot_demo(tools=tools, model_name=\"qwen2.5:3b\")\n",
    "    \n",
    "    # Launch interface\n",
    "    interface = chatbot.create_ui()\n",
    "    interface.launch(\n",
    "        share=False, \n",
    "        debug=True,\n",
    "        theme=gr.themes.Citrus()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4a719d-c2de-4a87-8d55-569b71889564",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e7a51f-a71a-4dbc-9b49-aea049fef05b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f73038-153d-4592-8ad1-9074fa10c914",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
